{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ananya023-2327/misc/blob/main/module_b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**TOPICS COVERED SO FAR:**\n",
        "1. Linear Regression\n",
        "2. Polynomial Regression\n",
        "3. Logistic Regression\n",
        "4. Regularization\n",
        "5. Dimensionality\n",
        "6. Gradient Descent\n",
        "7. Page Rank\n",
        "8. Support Vector Machines\n",
        "9. Neural Networks and Types\n",
        "10. Clustering\n",
        "11. Image Processing\n"
      ],
      "metadata": {
        "id": "rx9uaGzRa4Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression Example\n",
        "\n",
        "This example demonstrates how to use **linear regression** to learn a relationship between a single input variable `X` and an output variable `Y`.\n",
        "\n",
        "We generate a small dataset where the relationship is linear, and use **scikit-learn’s LinearRegression** to fit a model. The output includes:\n",
        "\n",
        "- A **scatter plot** of the training data in blue\n",
        "- The **fitted regression line** in red, representing the learned pattern\n",
        "\n",
        "This helps visualize how linear regression models try to minimize error by finding the best-fit line through the data points.\n"
      ],
      "metadata": {
        "id": "9ZdYqSVyy7yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (input X and output Y)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Input (features)\n",
        "Y = np.array([2, 4, 6, 8, 10])  # Output (labels)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, Y, color='blue', label=\"Training Data\")\n",
        "plt.plot(X, model.predict(X), color='red', label=\"Learned Pattern\")\n",
        "plt.xlabel(\"Input (X)\")\n",
        "plt.ylabel(\"Output (Y)\")\n",
        "plt.legend()\n",
        "plt.title(\"Simple Linear Regression: Learning Input-Output Relationship\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kqLUYu10c6Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Salary Based on Experience Using Linear Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2noF_j5zG4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic code\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# years of exp. and salary of 5 employees\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "Y = [30000, 35000, 45000, 55000, 70000]\n",
        "\n",
        "model = LinearRegression().fit(X, Y)\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X, model.predict(X))\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XGluP3nEc7_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define different values of b (intercept) with a fixed m\n",
        "b_values = [0, 2, 5]  # Different intercepts\n",
        "m = 1  # Fixed slope\n",
        "X = np.linspace(-5, 5, 100)  # Generate 100 values from -5 to 5\n",
        "\n",
        "# Plot different intercepts\n",
        "plt.figure(figsize=(8,6))\n",
        "for b in b_values:\n",
        "    Y = m * X + b\n",
        "    plt.plot(X, Y, label=f\"y = {m}x + {b}\")\n",
        "\n",
        "# Highlight origin and axes\n",
        "plt.axhline(0, color='black', linewidth=1)\n",
        "plt.axvline(0, color='black', linewidth=1)\n",
        "\n",
        "# Labels and legend\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Y values\")\n",
        "plt.title(\"Effect of Different Intercepts (b) on a Line\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ayKzOjGMc-3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define different values of m (slope) with a fixed b\n",
        "m_values = [0.5, 1, 2]  # Different slopes\n",
        "b = 0  # Fixed intercept\n",
        "X = np.linspace(-5, 5, 100)  # Generate 100 values from -5 to 5\n",
        "\n",
        "# Plot different slopes\n",
        "plt.figure(figsize=(8,6))\n",
        "for m in m_values:\n",
        "    Y = m * X + b\n",
        "    plt.plot(X, Y, label=f\"y = {m}x + {b}\")\n",
        "\n",
        "# Highlight origin and axes\n",
        "plt.axhline(0, color='black', linewidth=1)\n",
        "plt.axvline(0, color='black', linewidth=1)\n",
        "\n",
        "# Labels and legend\n",
        "plt.xlabel(\"X values\")\n",
        "plt.ylabel(\"Y values\")\n",
        "plt.title(\"Effect of Different Slopes (m) on a Line\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DnWVE2qodB_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (input X and output Y)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Input (features)\n",
        "Y = np.array([2, 4, 6, 8, 10])  # Output (labels) - follows Y = 2X\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "# Predict values\n",
        "X_test = np.array([6, 7, 8]).reshape(-1, 1)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X, Y, color='blue', label=\"Training Data\")\n",
        "plt.plot(X, model.predict(X), color='red', label=\"Learned Pattern (Y=2X)\")\n",
        "plt.scatter(X_test, predictions, color='green', marker='x', label=\"Predictions\")\n",
        "plt.xlabel(\"Input (X)\")\n",
        "plt.ylabel(\"Output (Y)\")\n",
        "plt.legend()\n",
        "plt.title(\"Simple Linear Regression: Learning Input-Output Relationship\")\n",
        "plt.show()\n",
        "\n",
        "# Print predictions\n",
        "for i, x in enumerate(X_test.flatten()):\n",
        "    print(f\"For input {x}, predicted output is {predictions[i]:.2f}\")\n"
      ],
      "metadata": {
        "id": "kiF-9wAfdHCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(20, 1) * 10  # 20 random values between 0 and 10\n",
        "y = 2 * X + 3 + np.random.randn(20, 1) * 2  # Linear function with noise\n",
        "\n",
        "# Train a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate MAE and MSE\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label=\"Regression Line\")\n",
        "plt.xlabel(\"X (Input Feature)\")\n",
        "plt.ylabel(\"y (Target Output)\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression with MAE & MSE Calculation\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5k_49n86dKm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Given data points\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [2, 3, 5, 4, 6]\n",
        "\n",
        "# Compute mean of x and y\n",
        "x_mean = sum(x) / len(x)\n",
        "y_mean = sum(y) / len(y)\n",
        "\n",
        "# Compute slope (m)\n",
        "numerator = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, y))\n",
        "denominator = sum((xi - x_mean) ** 2 for xi in x)\n",
        "m = numerator / denominator\n",
        "\n",
        "# Compute intercept (c)\n",
        "c = y_mean - m * x_mean\n",
        "\n",
        "# Print results\n",
        "print(f\"m = {m}\")\n",
        "print(f\"c = {c}\")\n"
      ],
      "metadata": {
        "id": "SscSpKpbdkUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given data points\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y_actual = np.array([2, 3, 5, 4, 6])\n",
        "\n",
        "# Regression equation: y = 0.9x + 1.3\n",
        "m = 0.9\n",
        "c = 1.3\n",
        "y_predicted = m * x + c  # Calculate predicted y values\n",
        "\n",
        "# Calculate errors (absolute differences)\n",
        "errors = abs(y_actual - y_predicted)\n",
        "\n",
        "# Plot actual points\n",
        "plt.scatter(x, y_actual, color='blue', label=\"Actual Points\")\n",
        "\n",
        "# Plot regression line\n",
        "plt.plot(x, y_predicted, color='red', label=\"Regression Line (y=0.9x+1.3)\")\n",
        "\n",
        "# Draw vertical lines for errors\n",
        "for i in range(len(x)):\n",
        "    plt.plot([x[i], x[i]], [y_actual[i], y_predicted[i]], 'g--', alpha=0.6)\n",
        "\n",
        "# Labels and legend\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear Regression with Error Visualization\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jyzR8KikdlPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(20, 1) * 10  # 20 random values between 0 and 10\n",
        "y = 2 * X + 3 + np.random.randn(20, 1) * 2  # Linear function with noise\n",
        "\n",
        "# Train a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Calculate MAE and MSE\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\\n\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_pred, color='red', linewidth=2, label=\"Regression Line\")\n",
        "plt.xlabel(\"X (Input Feature)\")\n",
        "plt.ylabel(\"y (Target Output)\")\n",
        "plt.legend()\n",
        "plt.title(\"Linear Regression with MAE & MSE Calculation\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R1BpAF-zdn_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate nonlinear data (quadratic relationship)\n",
        "np.random.seed(42)\n",
        "x = np.linspace(-5, 5, 50).reshape(-1, 1)  # 50 points between -5 and 5\n",
        "y = 3 * x**2 + 2 + np.random.normal(0, 3, size=x.shape)  # Quadratic with noise\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "# Calculate R² Score\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Plot the data and the linear fit\n",
        "plt.scatter(x, y, label='True Data', color='blue')\n",
        "plt.plot(x, y_pred, label='Linear Fit', color='red')\n",
        "plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"R² Value: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6TIzPO3eEVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Real-time example: Age vs. Reaction Time\n",
        "age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)\n",
        "reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(age, reaction_time)\n",
        "reaction_time_pred = model.predict(age)\n",
        "\n",
        "# Calculate R² Score\n",
        "r2 = r2_score(reaction_time, reaction_time_pred)\n",
        "\n",
        "# Plot the data and the linear fit\n",
        "plt.scatter(age, reaction_time, label='True Data', color='blue')\n",
        "plt.plot(age, reaction_time_pred, label='Linear Fit', color='red')\n",
        "plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Reaction Time (ms)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"R² Value: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "J8b0vN-2eFE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Real-time example: Age vs. Reaction Time\n",
        "age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)\n",
        "reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(age, reaction_time)\n",
        "reaction_time_pred_linear = linear_model.predict(age)\n",
        "linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)\n",
        "\n",
        "# Transform features for Polynomial Regression (Degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "age_poly = poly.fit_transform(age)\n",
        "polynomial_model = LinearRegression()\n",
        "polynomial_model.fit(age_poly, reaction_time)\n",
        "reaction_time_pred_poly = polynomial_model.predict(age_poly)\n",
        "poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)\n",
        "\n",
        "# Plot Linear and Polynomial Regression\n",
        "plt.scatter(age, reaction_time, label='True Data', color='blue')\n",
        "plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')\n",
        "plt.plot(age, reaction_time_pred_poly, label=f'Polynomial Fit (R²: {poly_r2:.4f})', color='green')\n",
        "plt.title('Linear vs. Polynomial Regression')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Reaction Time (ms)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Linear R² Value: {linear_r2:.4f}\")\n",
        "print(f\"Polynomial R² Value: {poly_r2:.4f}\")"
      ],
      "metadata": {
        "id": "l2o0BUh9eHRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define x values (range from -10 to 10)\n",
        "x = np.linspace(-10, 10, 100)\n",
        "\n",
        "# Generate random coefficients for each polynomial degree\n",
        "np.random.seed(42)  # For reproducibility\n",
        "coefficients = {\n",
        "    1: np.random.uniform(-5, 5, 2),  # Degree 1 (Linear)\n",
        "    2: np.random.uniform(-5, 5, 3),  # Degree 2 (Quadratic)\n",
        "    3: np.random.uniform(-5, 5, 4),  # Degree 3 (Cubic)\n",
        "    4: np.random.uniform(-5, 5, 5),  # Degree 4 (Quartic)\n",
        "    5: np.random.uniform(-5, 5, 6),  # Degree 5 (Quintic)\n",
        "}\n",
        "\n",
        "# Create separate plots for each polynomial degree\n",
        "for degree, coeffs in coefficients.items():\n",
        "    y = np.polyval(coeffs, x)  # Compute y values\n",
        "\n",
        "    plt.figure(figsize=(6, 4))  # Create a new figure for each plot\n",
        "    plt.plot(x, y, label=f'Degree {degree}', color='b')\n",
        "\n",
        "    # Plot settings\n",
        "    plt.axhline(0, color='black', linewidth=0.5, linestyle=\"--\")\n",
        "    plt.axvline(0, color='black', linewidth=0.5, linestyle=\"--\")\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title(f'Polynomial Regression (Degree {degree})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.show()  # Show each plot separately\n"
      ],
      "metadata": {
        "id": "GRhrfsbTeQOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Day Number\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "\n",
        "# Plant Growth\n",
        "y = np.array([1, 2, 3, 5, 15, 34, 48, 70, 136, 185])\n",
        "\n",
        "poly = PolynomialFeatures(degree = 8)\n",
        "x_poly = poly.fit_transform(x)\n",
        "\n",
        "\n",
        "# Create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(x_poly, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(x_poly)\n",
        "\n",
        "# Calculate R² score\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Print model details\n",
        "print(f\"Slope (coefficient): {model.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(x, y, color='green', label='Actual Growth')\n",
        "plt.plot(x, y_pred, color='blue', linestyle='--', label=f'Fitted Line (R² = {r2:.2f})')\n",
        "plt.xlabel('Day Number')\n",
        "plt.ylabel('Plant Growth')\n",
        "plt.title('Regression - Plant Growth Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ipTSrxvDeRIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I have data of GRE score and admitted or not. fit me a polynomial regression. Assume around 10 GRE scores and status. plot the graph. compute r2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "gre_scores = np.array([300, 310, 320, 330, 340, 350, 360, 370, 380, 390]).reshape(-1, 1)\n",
        "admitted = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1])  # 0: not admitted, 1: admitted\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=8)\n",
        "gre_poly = poly_features.fit_transform(gre_scores)\n",
        "\n",
        "# Fit the polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(gre_poly, admitted)\n",
        "\n",
        "# Make predictions\n",
        "gre_pred = np.linspace(min(gre_scores), max(gre_scores), 100).reshape(-1, 1)\n",
        "gre_pred_poly = poly_features.fit_transform(gre_pred)\n",
        "admitted_pred = model.predict(gre_pred_poly)\n",
        "\n",
        "# Calculate R-squared\n",
        "r2 = r2_score(admitted, model.predict(gre_poly))\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(gre_scores, admitted, label='Data')\n",
        "plt.plot(gre_pred, admitted_pred, color='red', label=f'Polynomial Regression (degree={8})')\n",
        "plt.xlabel('GRE Score')\n",
        "plt.ylabel('Admitted (0 or 1)')\n",
        "plt.title(f'Polynomial Regression (R-squared = {r2:.2f})')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hEFTl8QQrqIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate nonlinear data (quadratic relationship)\n",
        "np.random.seed(42)\n",
        "x = np.linspace(-5, 5, 50).reshape(-1, 1)  # 50 points between -5 and 5\n",
        "y = 3 * x**2 + 2 + np.random.normal(0, 3, size=x.shape)  # Quadratic with noise\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "# Calculate R² Score\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Plot the data and the linear fit\n",
        "plt.scatter(x, y, label='True Data', color='blue')\n",
        "plt.plot(x, y_pred, label='Linear Fit', color='red')\n",
        "plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"R² Value: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "KvMuOS45eYmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Real-time example: Age vs. Reaction Time (Nonlinear relationship)\n",
        "age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)\n",
        "reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "model = LinearRegression()\n",
        "model.fit(age, reaction_time)\n",
        "reaction_time_pred = model.predict(age)\n",
        "\n",
        "# Calculate R² Score\n",
        "r2 = r2_score(reaction_time, reaction_time_pred)\n",
        "\n",
        "# Plot the data and the linear fit\n",
        "plt.scatter(age, reaction_time, label='True Data', color='blue')\n",
        "plt.plot(age, reaction_time_pred, label='Linear Fit', color='red')\n",
        "plt.title(f'Linear Regression Fit (Low R² Value: {r2:.4f})')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Reaction Time (ms)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"R² Value: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "ky31eJ6necYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Step 2: Generate input values from -10 to 10\n",
        "z_values = np.linspace(-10, 10, 200)\n",
        "sigmoid_values = sigmoid(z_values)\n",
        "\n",
        "# Step 3: Plot the sigmoid function\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(z_values, sigmoid_values, label=\"Sigmoid Function\", color=\"darkblue\")\n",
        "plt.title(\"Sigmoid Function Curve\")\n",
        "plt.xlabel(\"Input (z)\")\n",
        "plt.ylabel(\"Sigmoid(z)\")\n",
        "plt.grid(True)\n",
        "plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.axhline(0.5, color='red', linestyle='--', alpha=0.6, label='Threshold at 0.5')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UPhklowYefAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#  Create a simple dataset (Hours Studied vs Pass/Fail)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Hours studied\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict pass/fail for a new student who studied for given hours\n",
        "# new_hours = np.array([[]])\n",
        "# prediction = model.predict(new_hours)\n",
        "# probability = model.predict_proba(new_hours)[0][1]  # Probability of passing\n",
        "\n",
        "# print(f\"Predicted Outcome: {'Pass' if prediction[0] == 1 else 'Fail'}\")\n",
        "# print(f\"Probability of Passing: {probability:.2f}\")\n",
        "\n",
        "# Plot the decision boundary\n",
        "X_test = np.linspace(0, 10, 100).reshape(-1, 1)  # Generate values from 0 to 10\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities\n",
        "\n",
        "plt.scatter(X, y, color=\"blue\", label=\"Training Data\")\n",
        "plt.plot(X_test, y_prob, color=\"red\", label=\"Sigmoid Curve\")\n",
        "plt.axhline(0.5, linestyle=\"--\", color=\"gray\", label=\"Decision Boundary\")\n",
        "plt.xlabel(\"Hours Studied\")\n",
        "plt.ylabel(\"Probability of Passing\")\n",
        "plt.legend()\n",
        "plt.title(\"Logistic Regression: Predicting Exam Pass/Fail\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M4rYmqS9ehtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Real-time example: Age vs. Reaction Time\n",
        "age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)\n",
        "reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(age, reaction_time)\n",
        "reaction_time_pred_linear = linear_model.predict(age)\n",
        "linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)\n",
        "\n",
        "# Transform features for Polynomial Regression (Degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "age_poly = poly.fit_transform(age)\n",
        "polynomial_model = LinearRegression()\n",
        "polynomial_model.fit(age_poly, reaction_time)\n",
        "reaction_time_pred_poly = polynomial_model.predict(age_poly)\n",
        "poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)\n",
        "\n",
        "# Plot Linear and Polynomial Regression\n",
        "plt.scatter(age, reaction_time, label='True Data', color='blue')\n",
        "plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')\n",
        "plt.plot(age, reaction_time_pred_poly, label=f'Polynomial Fit (R²: {poly_r2:.4f})', color='green')\n",
        "plt.title('Linear vs. Polynomial Regression')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Reaction Time (ms)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Linear R² Value: {linear_r2:.4f}\")\n",
        "print(f\"Polynomial R² Value: {poly_r2:.4f}\")"
      ],
      "metadata": {
        "id": "ehph_E1Dem8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Real-time example: Age vs. Reaction Time\n",
        "age = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]).reshape(-1, 1)\n",
        "reaction_time = np.array([500, 480, 450, 430, 420, 415, 420, 430, 450, 480, 520, 570, 630])  # Reaction time in ms\n",
        "\n",
        "# Fit a Linear Regression Model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(age, reaction_time)\n",
        "reaction_time_pred_linear = linear_model.predict(age)\n",
        "linear_r2 = r2_score(reaction_time, reaction_time_pred_linear)\n",
        "\n",
        "# Fit Polynomial Regression Models with Different Degrees\n",
        "degrees = [2, 4, 6]\n",
        "plt.scatter(age, reaction_time, label='True Data', color='blue')\n",
        "colors = ['green', 'purple', 'orange']\n",
        "\n",
        "for i, degree in enumerate(degrees):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    age_poly = poly.fit_transform(age)\n",
        "    polynomial_model = LinearRegression()\n",
        "    polynomial_model.fit(age_poly, reaction_time)\n",
        "    reaction_time_pred_poly = polynomial_model.predict(age_poly)\n",
        "    poly_r2 = r2_score(reaction_time, reaction_time_pred_poly)\n",
        "\n",
        "    plt.plot(age, reaction_time_pred_poly, label=f'Poly Deg {degree} (R²: {poly_r2:.4f})', color=colors[i])\n",
        "    print(f\"Polynomial Degree {degree} R² Value: {poly_r2:.4f}\\n\")\n",
        "\n",
        "# Plot Linear Regression\n",
        "plt.plot(age, reaction_time_pred_linear, label=f'Linear Fit (R²: {linear_r2:.4f})', color='red')\n",
        "\n",
        "plt.title('Model Complexity: Underfitting vs. Overfitting')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Reaction Time (ms)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n Linear R² Value: {linear_r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "WNLMyofCepzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Sample Nonlinear Data\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "y = np.array([2, 6, 15, 30, 55, 90, 140, 210, 300, 410])  # Quadratic pattern\n",
        "\n",
        "# Transform X for Polynomial Regression (Degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the Polynomial Regression Model\n",
        "model = LinearRegression().fit(X_poly, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "# Evaluation Metrics\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "# Print Metrics\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Plot the Results\n",
        "plt.scatter(X, y, color='blue', label=\"Actual Data\")\n",
        "plt.plot(X, y_pred, color='red', label=\"Polynomial Regression\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Polynomial Regression with Performance Metrics\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e0-b3zoJetHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#  Create a simple dataset (Hours Studied vs Pass/Fail)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Hours studied\n",
        "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(X, y, color='red', label='Actual Growth')\n",
        "plt.plot(X, y_pred, color='blue', linestyle='--')\n",
        "plt.xlabel('Hours Studies')\n",
        "plt.ylabel('Pass/Fail')\n",
        "plt.title('Student Results')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "71LGkna5nEM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiToOfcSVVKC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Step 1: Create a simple dataset (2 features, 2 classes)\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
        "                           n_informative=2, n_clusters_per_class=1, random_state=1)\n",
        "\n",
        "# Step 2: Fit logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Step 3: Plot the data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')\n",
        "plt.title(\"Logistic Regression Demo\")\n",
        "plt.xlabel(\"Category 1\")\n",
        "plt.ylabel(\"Category 2\")\n",
        "\n",
        "# Step 4: Plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                     np.linspace(y_min, y_max, 200))\n",
        "\n",
        "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "  return 1/(1 + np.exp(-z))\n",
        "\n",
        "# Step 2: Generate input values from -10 to 10\n",
        "z_values = np.linspace(-10, 10, 200)\n",
        "sigmoid_values = sigmoid(z_values)\n",
        "\n",
        "# Step 3: Plot the sigmoid function\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(z_values, sigmoid_values, label=\"Sigmoid Function\", color=\"darkblue\")\n",
        "plt.title(\"Sigmoid Function Curve\")\n",
        "plt.xlabel(\"Input (z)\")\n",
        "plt.ylabel(\"Sigmoid(z)\")\n",
        "plt.grid(True)\n",
        "plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.axhline(0.5, color='red', linestyle='--', alpha=0.6, label='Threshold at 0.5')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OwHt8VLqVtTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Sample data: Hours studied vs marks (for linear) and pass/fail (for logistic)\n",
        "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
        "y_regression = np.array([35, 40, 50, 55, 60, 65, 70, 75, 85, 90])  # marks\n",
        "y_classification = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])        # pass/fail\n",
        "\n",
        "# Train the model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y_regression)\n",
        "y_linear_pred = linear_model.predict(X)\n",
        "\n",
        "# Train the model\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X, y_classification)\n",
        "x_range = np.linspace(0, 11, 300).reshape(-1, 1)\n",
        "y_logistic_prob = logistic_model.predict_proba(x_range)[:, 1]\n",
        "\n",
        "# Plotting\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Linear Regression Plot\n",
        "ax1.scatter(X, y_regression, color='blue', label='Actual Marks')\n",
        "ax1.plot(X, y_linear_pred, color='red', label='Linear Fit')\n",
        "ax1.set_title('Linear Regression: Hours vs Marks')\n",
        "ax1.set_xlabel('Hours Studied')\n",
        "ax1.set_ylabel('Marks Scored')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Logistic Regression Plot\n",
        "ax2.scatter(X, y_classification, color='green', label='Actual Pass/Fail')\n",
        "ax2.plot(x_range, y_logistic_prob, color='purple', label='Logistic Curve')\n",
        "ax2.axhline(0.5, color='gray', linestyle='--', label='Threshold = 0.5')\n",
        "ax2.set_title('Logistic Regression: Hours vs Pass Probability')\n",
        "ax2.set_xlabel('Hours Studied')\n",
        "ax2.set_ylabel('Probability of Passing')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EZG5QkcfpyqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Data\n",
        "gre_scores = np.array([290, 300, 310, 320, 330, 340, 295, 305, 315, 325])\n",
        "admissions = np.array([0, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
        "\n",
        "X = gre_scores.reshape(-1, 1)\n",
        "y = admissions\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Widget\n",
        "gre_input = widgets.IntSlider(\n",
        "    value=310,\n",
        "    min=280,\n",
        "    max=350,\n",
        "    step=1,\n",
        "    description='GRE Score:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "# Function to update plot and prediction\n",
        "def update_plot(gre_val):\n",
        "    prediction = model.predict([[gre_val]])[0]\n",
        "    prob = model.predict_proba([[gre_val]])[0][1]\n",
        "\n",
        "    print(f\"\\nGRE Score: {gre_val}\")\n",
        "    print(f\"Prediction: {'Admitted' if prediction else 'Not Admitted'}\")\n",
        "    print(f\"Probability of Admission: {prob:.2f}\")\n",
        "\n",
        "    # Plot sigmoid\n",
        "    gre_range = np.linspace(280, 350, 300).reshape(-1, 1)\n",
        "    predicted_probs = model.predict_proba(gre_range)[:, 1]\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(gre_scores, admissions, color='red', edgecolors='k', label='Training Data')\n",
        "    plt.plot(gre_range, predicted_probs, color='blue', label='Sigmoid Curve')\n",
        "    plt.axvline(gre_val, color='green', linestyle='--', label='Your GRE')\n",
        "    plt.xlabel(\"GRE Score\")\n",
        "    plt.ylabel(\"Probability of Admission\")\n",
        "    plt.title(\"Logistic Regression Prediction\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Display the widget\n",
        "widgets.interact(update_plot, gre_val=gre_input)\n"
      ],
      "metadata": {
        "id": "vsjJF63esEL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# University Admission Prediction using Logistic Regression\n",
        "\n",
        "In this project, we use **logistic regression** to predict whether a student will be admitted to a university based on their **GRE score** and **CGPA**.\n",
        "\n",
        "The model is trained on a small dataset of past applicants. It outputs both the **prediction** (_Admitted_ or _Not Admitted_) and the **probability of admission**.\n",
        "\n",
        "A visualization displays:\n",
        "- Admitted applicants in **green**\n",
        "- Non-admitted applicants in **red**\n",
        "- Your input point in **blue** on a 2D plot\n",
        "\n"
      ],
      "metadata": {
        "id": "NZ5XzEJXwy_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample data: GRE, CGPA, Admission (1 = Admitted, 0 = Not Admitted)\n",
        "data = np.array([\n",
        "    [310, 8.0, 0],\n",
        "    [320, 8.5, 1],\n",
        "    [330, 9.0, 1],\n",
        "    [300, 7.5, 0],\n",
        "    [340, 9.5, 1],\n",
        "    [305, 6.5, 0],\n",
        "    [315, 7.8, 0],\n",
        "    [325, 8.7, 1],\n",
        "    [310, 9.0, 1],\n",
        "    [295, 6.8, 0]\n",
        "])\n",
        "\n",
        "# Split features and labels\n",
        "X = data[:, :2]  # GRE and CGPA\n",
        "y = data[:, 2]   # Admission\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Take user input\n",
        "gre_input = float(input(\"Enter your GRE score: \"))\n",
        "cgpa_input = float(input(\"Enter your CGPA: \"))\n",
        "user_input = np.array([[gre_input, cgpa_input]])\n",
        "\n",
        "# Predict admission\n",
        "prediction = model.predict(user_input)\n",
        "probability = model.predict_proba(user_input)[0][1]\n",
        "\n",
        "# Output result\n",
        "print(\"\\nPrediction Result:\")\n",
        "print(\"\\nAdmitted\" if prediction[0] == 1 else \"Not Admitted \")\n",
        "print(f\"\\nProbability of Admission: {probability:.2f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8,6))\n",
        "for admitted in [0, 1]:\n",
        "    subset = X[y == admitted]\n",
        "    label = 'Admitted' if admitted == 1 else 'Not Admitted'\n",
        "    color = 'green' if admitted == 1 else 'red'\n",
        "    plt.scatter(subset[:, 0], subset[:, 1], label=label, c=color, edgecolor='k')\n",
        "\n",
        "# User point\n",
        "plt.scatter(gre_input, cgpa_input, c='blue', s=100, edgecolors='k', label=\"You\")\n",
        "\n",
        "plt.xlabel(\"GRE Score\")\n",
        "plt.ylabel(\"CGPA\")\n",
        "plt.title(\"University Admission Prediction\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g8A4p2-v6B3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Sample data: GRE, CGPA, Admitted\n",
        "data = [\n",
        "    [310, 8.0, 1],\n",
        "    [300, 7.5, 0],\n",
        "    [320, 8.6, 1],\n",
        "    [315, 8.2, 1],\n",
        "    [299, 7.0, 0],\n",
        "    [325, 9.0, 1],\n",
        "    [280, 6.5, 0],\n",
        "    [330, 9.1, 1],\n",
        "    [340, 9.5, 1],\n",
        "    [290, 7.2, 0],\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"GRE\", \"CGPA\", \"Admitted\"])\n",
        "\n",
        "# Features and label\n",
        "X = df[[\"GRE\", \"CGPA\"]]\n",
        "y = df[\"Admitted\"]\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Extract weights\n",
        "w1, w2 = model.coef_[0]\n",
        "b = model.intercept_[0]\n",
        "\n",
        "print(f\"\\nModel Weights:\")\n",
        "print(f\"w1 (GRE): {w1:.4f}\")\n",
        "print(f\"w2 (CGPA): {w2:.4f}\")\n",
        "print(f\"Intercept (b): {b:.4f}\")\n",
        "\n",
        "# Take user input\n",
        "gre_input = float(input(\"\\nEnter GRE score: \"))\n",
        "cgpa_input = float(input(\"Enter CGPA: \"))\n",
        "user_input = pd.DataFrame({\"GRE\": [gre_input], \"CGPA\": [cgpa_input]})\n",
        "\n",
        "# Prediction\n",
        "probability = model.predict_proba(user_input)[0][1]\n",
        "prediction = model.predict(user_input)[0]\n",
        "\n",
        "print(f\"\\nPredicted probability of admission: {probability:.4f}\")\n",
        "print(\"Prediction:\", \"Admitted ✅\" if prediction == 1 else \"Not Admitted ❌\")\n",
        "\n",
        "# Plot decision boundary\n",
        "gre_vals = np.linspace(280, 340, 100)\n",
        "cgpa_vals = np.linspace(6.0, 10.0, 100)\n",
        "GRE_grid, CGPA_grid = np.meshgrid(gre_vals, cgpa_vals)\n",
        "grid_df = pd.DataFrame({\n",
        "    \"GRE\": GRE_grid.ravel(),\n",
        "    \"CGPA\": CGPA_grid.ravel()\n",
        "})\n",
        "Z = model.predict_proba(grid_df)[:, 1].reshape(GRE_grid.shape)\n",
        "\n",
        "plt.contourf(GRE_grid, CGPA_grid, Z, levels=[0, 0.5, 1], alpha=0.3, colors=[\"red\", \"green\"])\n",
        "plt.scatter(df[\"GRE\"], df[\"CGPA\"], c=df[\"Admitted\"], cmap=\"bwr\", edgecolors='k')\n",
        "plt.scatter(gre_input, cgpa_input, color='gold', s=100, edgecolors='black', label=\"Your Input\")\n",
        "plt.xlabel(\"GRE Score\")\n",
        "plt.ylabel(\"CGPA\")\n",
        "plt.title(\"Admission Prediction using Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgB-Utxk620S",
        "outputId": "b8950184-81bd-433d-fee1-4a6fb7cc01c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Weights:\n",
            "w1 (GRE): 0.6091\n",
            "w2 (CGPA): 0.0394\n",
            "Intercept (b): -186.2807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REGULARIZATION\n",
        "#===============\n",
        "\n",
        "# The Obervations we made in the spreadsheet, how does it boil down in the code?\n",
        "# How do we compute the lasso and ridge values using inbuilt libraries?\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "# X has two features (like hours of study and sleep)\n",
        "X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])\n",
        "# y is the score\n",
        "y = np.array([2, 4, 7, 8])\n",
        "\n",
        "# Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Ridge Regression (L2 Regularization)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "# Lasso Regression (L1 Regularization)\n",
        "lasso = Lasso(alpha=1.0)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Ridge Regression Coefficients :\", ridge.coef_)\n",
        "print(\"Lasso Regression Coefficients :\", lasso.coef_)\n"
      ],
      "metadata": {
        "id": "ZTtG4Xk70-SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working on our case study and finding R2 score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Input features: hours of study and sleep\n",
        "X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])\n",
        "y = np.array([2, 4, 7, 8])  # Target: score\n",
        "\n",
        "\n",
        "# 1. Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X,y)\n",
        "y_pred_linear = lr.predict(X)\n",
        "print(\"Linear Regression R² Score:\", r2_score(y, y_pred_linear))\n",
        "\n",
        "\n",
        "# 2. Polynomial Regression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "poly_lr = LinearRegression()\n",
        "poly_lr.fit(X_poly, y)\n",
        "y_pred_poly = poly_lr.predict(X_poly)\n",
        "print(\"Polynomial Regression R² Score:\", r2_score(y, y_pred_poly))"
      ],
      "metadata": {
        "id": "VmNtst-VbNUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting from our case study\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Input features: hours of study and sleep\n",
        "X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])\n",
        "y = np.array([2, 4, 7, 8])  # Target: score\n",
        "\n",
        "# 1. Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "y_pred_linear = lr.predict(X)\n",
        "\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Linear Regression R² Score:\", r2_score(y, y_pred_linear))\n",
        "\n",
        "# 2. Polynomial Regression\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "poly_lr = LinearRegression()\n",
        "poly_lr.fit(X_poly, y)\n",
        "y_pred_poly = poly_lr.predict(X_poly)\n",
        "\n",
        "print(\"Polynomial Regression R² Score:\", r2_score(y, y_pred_poly))\n",
        "\n",
        "# 3. Predict on a new sample\n",
        "new_sample = np.array([[5,6]])\n",
        "new_sample_poly = poly.transform(new_sample)\n",
        "\n",
        "print(\"Prediction on new sample:\")\n",
        "print(\"- Linear:\", lr.predict(new_sample))\n",
        "print(\"- Polynomial:\", poly_lr.predict(new_sample_poly))\n"
      ],
      "metadata": {
        "id": "MT4vZiMIWWsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with all possilble cases\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Input features: hours of study and sleep\n",
        "X = np.array([[1, 5], [2, 5], [3, 5], [4, 6]])\n",
        "y = np.array([2, 4, 7, 8])  # Target: score\n",
        "\n",
        "# 1. Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "y_pred_linear = lr.predict(X)\n",
        "\n",
        "# 2. Ridge Regression (linear features)\n",
        "ridge_linear = Ridge(alpha=1.0)\n",
        "ridge_linear.fit(X, y)\n",
        "y_pred_ridge_linear = ridge_linear.predict(X)\n",
        "\n",
        "# 3. Lasso Regression (linear features)\n",
        "lasso_linear = Lasso(alpha=1.0)\n",
        "lasso_linear.fit(X, y)\n",
        "y_pred_lasso_linear = lasso_linear.predict(X)\n",
        "\n",
        "print(\"=== Linear Features ===\")\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Linear R² Score:\", r2_score(y, y_pred_linear))\n",
        "print(\"Ridge Coefficients:\", ridge_linear.coef_)\n",
        "print(\"Ridge R² Score:\", r2_score(y, y_pred_ridge_linear))\n",
        "print(\"Lasso Coefficients:\", lasso_linear.coef_)\n",
        "print(\"Lasso R² Score:\", r2_score(y, y_pred_lasso_linear))\n",
        "\n",
        "# 4. Polynomial Regression (degree 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# 4a. Linear Regression on Polynomial Features\n",
        "poly_lr = LinearRegression()\n",
        "poly_lr.fit(X_poly, y)\n",
        "y_pred_poly = poly_lr.predict(X_poly)\n",
        "\n",
        "# 4b. Ridge Regression on Polynomial Features\n",
        "ridge_poly = Ridge(alpha=1.0)\n",
        "ridge_poly.fit(X_poly, y)\n",
        "y_pred_ridge_poly = ridge_poly.predict(X_poly)\n",
        "\n",
        "# 4c. Lasso Regression on Polynomial Features\n",
        "lasso_poly = Lasso(alpha=1.0, max_iter=10000)\n",
        "lasso_poly.fit(X_poly, y)\n",
        "y_pred_lasso_poly = lasso_poly.predict(X_poly)\n",
        "\n",
        "print(\"\\n=== Polynomial Features ===\")\n",
        "print(\"Polynomial Regression Coefficients:\", poly_lr.coef_)\n",
        "print(\"Polynomial R² Score:\", r2_score(y, y_pred_poly))\n",
        "print(\"Ridge Coefficients:\", ridge_poly.coef_)\n",
        "print(\"Ridge R² Score:\", r2_score(y, y_pred_ridge_poly))\n",
        "print(\"Lasso Coefficients:\", lasso_poly.coef_)\n",
        "print(\"Lasso R² Score:\", r2_score(y, y_pred_lasso_poly))\n",
        "\n",
        "# 5. Prediction on new sample [5, 6]\n",
        "new_sample = np.array([[5, 6]])\n",
        "new_sample_poly = poly.transform(new_sample)\n",
        "\n",
        "print(\"\\n=== Prediction on [5, 6] ===\")\n",
        "print(\"Linear:\", lr.predict(new_sample))\n",
        "print(\"Linear Ridge:\", ridge_linear.predict(new_sample))\n",
        "print(\"Linear Lasso:\", lasso_linear.predict(new_sample))\n",
        "print(\"Polynomial:\", poly_lr.predict(new_sample_poly))\n",
        "print(\"Poly Ridge:\", ridge_poly.predict(new_sample_poly))\n",
        "print(\"Poly Lasso:\", lasso_poly.predict(new_sample_poly))\n"
      ],
      "metadata": {
        "id": "xUndH0riWslP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#---------------------------------\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (6 data points, 2 features)\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 3],\n",
        "    [4, 4],\n",
        "    [5, 5],\n",
        "    [6, 6]\n",
        "])\n",
        "\n",
        "# Output (some linear relation + noise)\n",
        "y = np.array([3, 5, 7, 9, 11, 13])\n",
        "\n",
        "# Model 1: Linear Regression (no regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "y_pred_lr = lr.predict(X)\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lr))\n",
        "\n",
        "# Model 2: Ridge Regression\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X, y)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "print(\"\\nRidge Coefficients:\", ridge.coef_)\n",
        "print(\"MSE (Ridge):\", mean_squared_error(y, y_pred_ridge))\n",
        "\n",
        "# Model 3: Lasso Regression\n",
        "lasso = Lasso(alpha=1.0)\n",
        "lasso.fit(X, y)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "print(\"\\nLasso Coefficients:\", lasso.coef_)\n",
        "print(\"MSE (Lasso):\", mean_squared_error(y, y_pred_lasso))\n"
      ],
      "metadata": {
        "id": "78PeGEgtP_0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of given problem\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Sample dataset (6 points, 2 features)\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 3],\n",
        "    [4, 4],\n",
        "    [5, 5],\n",
        "    [6, 6]\n",
        "])\n",
        "\n",
        "# Target variable (roughly y = 1*x1 + 1*x2)\n",
        "y = np.array([3, 5, 7, 9, 11, 13])\n",
        "\n",
        "# Train all three models\n",
        "lr = LinearRegression().fit(X, y)\n",
        "ridge = Ridge(alpha=1.0).fit(X, y)\n",
        "lasso = Lasso(alpha=1.0).fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "\n",
        "# Print coefficients and errors\n",
        "print(\"Linear Regression Coefficients:\", lr.coef_, \"MSE:\", mean_squared_error(y, y_pred_lr))\n",
        "print(\"Ridge Regression Coefficients:\", ridge.coef_, \"MSE:\", mean_squared_error(y, y_pred_ridge))\n",
        "print(\"Lasso Regression Coefficients:\", lasso.coef_, \"MSE:\", mean_squared_error(y, y_pred_lasso))\n",
        "\n",
        "# Plotting predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y, label='True Values', marker='o', linewidth=2)\n",
        "plt.plot(y_pred_lr, label='Linear Regression', marker='s')\n",
        "plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')\n",
        "plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')\n",
        "\n",
        "plt.title(\"Comparison of Linear, Ridge, and Lasso Regression\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_dBfqzUgQNBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets add R2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 3],\n",
        "    [4, 4],\n",
        "    [5, 5],\n",
        "    [6, 6]\n",
        "])\n",
        "\n",
        "y = np.array([3, 5, 7, 9, 11, 13])\n",
        "\n",
        "# Fit models\n",
        "lr = LinearRegression().fit(X, y)\n",
        "ridge = Ridge(alpha=1.0).fit(X, y)\n",
        "lasso = Lasso(alpha=1.0).fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "\n",
        "# Print metrics\n",
        "print(\"=== Linear Regression ===\")\n",
        "print(\"Coefficients:\", lr.coef_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lr))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lr))\n",
        "\n",
        "print(\"\\n=== Ridge Regression ===\")\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_ridge))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_ridge))\n",
        "\n",
        "print(\"\\n=== Lasso Regression ===\")\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lasso))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lasso))\n",
        "\n"
      ],
      "metadata": {
        "id": "4U0z4SCKQa8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets add prediction\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# === Step 1: Create Dataset ===\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 3],\n",
        "    [4, 4],\n",
        "    [5, 5],\n",
        "    [6, 6]\n",
        "])\n",
        "y = np.array([3, 5, 7, 9, 11, 13])  # Simple linear target: y = x1 + x2 + 1\n",
        "\n",
        "# === Step 2: Train Models ===\n",
        "lr = LinearRegression().fit(X, y)\n",
        "ridge = Ridge(alpha=1.0).fit(X, y)\n",
        "lasso = Lasso(alpha=1.0).fit(X, y)\n",
        "\n",
        "# === Step 3: Make Predictions on Training Data ===\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "\n",
        "# === Step 4: Show Coefficients, MSE, and R² Scores ===\n",
        "print(\"=== Linear Regression ===\")\n",
        "print(\"Coefficients:\", lr.coef_)\n",
        "print(\"Intercept:\", lr.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lr))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lr))\n",
        "\n",
        "print(\"\\n=== Ridge Regression ===\")\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "print(\"Intercept:\", ridge.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_ridge))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_ridge))\n",
        "\n",
        "print(\"\\n=== Lasso Regression ===\")\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "print(\"Intercept:\", lasso.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lasso))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lasso))\n",
        "\n",
        "# === Step 5: Plot the Predictions ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y, label='True Values', marker='o', linewidth=2)\n",
        "plt.plot(y_pred_lr, label='Linear Regression', marker='s')\n",
        "plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')\n",
        "plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')\n",
        "plt.title(\"Predictions: Linear vs Ridge vs Lasso\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Step 6: Predict on New Input ===\n",
        "new_sample = np.array([[7, 7]])\n",
        "\n",
        "pred_lr = lr.predict(new_sample)\n",
        "pred_ridge = ridge.predict(new_sample)\n",
        "pred_lasso = lasso.predict(new_sample)\n",
        "\n",
        "print(\"\\n=== Prediction for New Input [7, 7] ===\")\n",
        "print(\"Linear Regression Prediction:\", pred_lr[0])\n",
        "print(\"Ridge Regression Prediction:\", pred_ridge[0])\n",
        "print(\"Lasso Regression Prediction:\", pred_lasso[0])\n"
      ],
      "metadata": {
        "id": "dCwV9a7JQ8kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Where is the lowest Point?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bowl-shaped curve\n",
        "x = np.linspace(-3, 3, 100)\n",
        "y = x**2\n",
        "\n",
        "# Plot curve\n",
        "plt.plot(x, y, label=\"y = x²\", color='darkgreen')\n",
        "\n",
        "# Mark the lowest point\n",
        "plt.plot(0, 0, 'ro')\n",
        "plt.text(0, 0.5, \"Lowest point\", ha='center', color='red')\n",
        "\n",
        "# Add question\n",
        "plt.title(\"Where’s the Lowest Point?\")\n",
        "\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ebD5x7BkdTI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Define function and its gradient\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "def grad_f(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Hiker A\n",
        "x0_a = 3.0\n",
        "alpha_a = 0.1\n",
        "\n",
        "# Hiker B\n",
        "x0_b = -2.5\n",
        "alpha_b = 0.05\n",
        "\n",
        "steps = 30\n",
        "\n",
        "# Gradient Descent Paths\n",
        "x_vals_a = [x0_a]\n",
        "x_vals_b = [x0_b]\n",
        "\n",
        "for _ in range(steps):\n",
        "    x_next_a = x_vals_a[-1] - alpha_a * grad_f(x_vals_a[-1])\n",
        "    x_next_b = x_vals_b[-1] - alpha_b * grad_f(x_vals_b[-1])\n",
        "    x_vals_a.append(x_next_a)\n",
        "    x_vals_b.append(x_next_b)\n",
        "\n",
        "y_vals_a = [f(x) for x in x_vals_a]\n",
        "y_vals_b = [f(x) for x in x_vals_b]\n",
        "\n",
        "# Set up the figure\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "x_curve = np.linspace(-4, 4, 400)\n",
        "y_curve = f(x_curve)\n",
        "ax.plot(x_curve, y_curve, 'gray', label=\"y = x²\")\n",
        "\n",
        "point_a, = ax.plot([], [], 'ro', label='Hiker A (α=0.1)')\n",
        "point_b, = ax.plot([], [], 'bo', label='Hiker B (α=0.05)')\n",
        "\n",
        "text_a = ax.text(-3.5, 10, '', fontsize=9, color='red')\n",
        "text_b = ax.text(-3.5, 8.5, '', fontsize=9, color='blue')\n",
        "\n",
        "ax.set_xlim(-4, 4)\n",
        "ax.set_ylim(0, 12)\n",
        "ax.set_xlabel(\"x\")\n",
        "ax.set_ylabel(\"y\")\n",
        "ax.set_title(\"Gradient Descent: Two Hikers Going Down the Valley\")\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "# Animation update function\n",
        "def update(i):\n",
        "    point_a.set_data([x_vals_a[i]], [y_vals_a[i]])\n",
        "    point_b.set_data([x_vals_b[i]], [y_vals_b[i]])\n",
        "    text_a.set_text(f\"Hiker A Step {i}: x={x_vals_a[i]:.2f}\")\n",
        "    text_b.set_text(f\"Hiker B Step {i}: x={x_vals_b[i]:.2f}\")\n",
        "    return point_a, point_b, text_a, text_b\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update, frames=steps+1, interval=400, blit=False)\n",
        "\n",
        "# Show animation inline in Colab\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "metadata": {
        "id": "yfOEbznxeFeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function and its derivative\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "def grad_f(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Starting point\n",
        "x_start = 2.5\n",
        "\n",
        "# Learning rates to compare\n",
        "learning_rates = [0.01, 0.1, 0.5]\n",
        "colors = ['blue', 'green', 'red']\n",
        "labels = ['Small Step Hiker (0.01)', 'Medium Step Hiker (0.1)', 'Large Step Hiker (0.5)']\n",
        "\n",
        "# Plot the function\n",
        "x = np.linspace(-3, 3, 400)\n",
        "y = f(x)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'k-', label='y = x²')\n",
        "plt.axhline(0, color='gray', lw=0.5)\n",
        "plt.axvline(0, color='gray', lw=0.5)\n",
        "\n",
        "# Show each learning rate path with arrows\n",
        "for lr, color, label in zip(learning_rates, colors, labels):\n",
        "    x_vals = [x_start]\n",
        "    for _ in range(5):\n",
        "        x_new = x_vals[-1] - lr * grad_f(x_vals[-1])\n",
        "        x_vals.append(x_new)\n",
        "\n",
        "    y_vals = [f(xi) for xi in x_vals]\n",
        "    plt.plot(x_vals, y_vals, 'o-', color=color, label=label)\n",
        "\n",
        "    # Draw arrows showing steps\n",
        "    for i in range(len(x_vals) - 1):\n",
        "        dx = x_vals[i+1] - x_vals[i]\n",
        "        dy = y_vals[i+1] - y_vals[i]\n",
        "        plt.arrow(x_vals[i], y_vals[i], dx, dy, color=color, head_width=0.1, length_includes_head=True)\n",
        "\n",
        "# Final formatting\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y = x²\")\n",
        "plt.title(\"Step Size and Direction\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p3E32AVshMEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A quadratic cost function: y = (x - 3)^2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def func(x):\n",
        "    return (x - 3)**2\n",
        "\n",
        "def grad(x):\n",
        "    return 2*(x - 3)\n",
        "\n",
        "x = 0  # starting point\n",
        "lr = 0.15\n",
        "steps = []\n",
        "for i in range(20):\n",
        "    x = x - lr * grad(x)\n",
        "    steps.append(x)\n",
        "\n",
        "xs = [i/10 for i in range(-10, 60)]\n",
        "ys = [func(i) for i in xs]\n",
        "\n",
        "plt.plot(xs, ys, label='Function')\n",
        "plt.plot(steps, [func(i) for i in steps], 'ro--', label='Steps')\n",
        "plt.legend()\n",
        "plt.title(\"Gradient Descent on a Simple Function\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cYcGBCAXmg-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Function and its gradient\n",
        "def func(x):\n",
        "    return (x - 3)**2\n",
        "\n",
        "def grad(x):\n",
        "    return 2*(x - 3)\n",
        "\n",
        "# Main plot function\n",
        "def plot_gradient_descent(lr=0.1):\n",
        "    x = 0\n",
        "    steps = [x]\n",
        "    for _ in range(20):\n",
        "        x = x - lr * grad(x)\n",
        "        steps.append(x)\n",
        "\n",
        "    xs = np.linspace(-1, 6, 100)\n",
        "    ys = func(xs)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(xs, ys, label=\"Function: (x - 3)^2\")\n",
        "    plt.plot(steps, [func(s) for s in steps], 'ro--', label=\"Gradient Descent Steps\")\n",
        "    plt.title(f\"Gradient Descent | Learning Rate: {lr}\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"f(x)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Create slider\n",
        "interact(plot_gradient_descent, lr=widgets.FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01))"
      ],
      "metadata": {
        "id": "7XKimrVCmsjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function and derivative\n",
        "def f(x):\n",
        "    return (x - 3)**2\n",
        "\n",
        "def f_prime(x):\n",
        "    return 2 * (x - 3)\n",
        "\n",
        "# x range for plotting the function\n",
        "x_vals = np.linspace(-1, 7, 300)\n",
        "y_vals = f(x_vals)\n",
        "\n",
        "# Points of interest\n",
        "points = [1.5, 3, 4.5]  # x < 3, x = 3, x > 3\n",
        "\n",
        "# Plot the function\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_vals, y_vals, label='$f(x) = (x - 3)^2$', color='blue')\n",
        "plt.axvline(3, linestyle='--', color='gray', alpha=0.6)\n",
        "\n",
        "# Plot tangent lines at selected points\n",
        "for x0 in points:\n",
        "    slope = f_prime(x0)\n",
        "    y0 = f(x0)\n",
        "    tangent_x = np.linspace(x0 - 1, x0 + 1, 10)\n",
        "    tangent_y = slope * (tangent_x - x0) + y0\n",
        "    plt.plot(tangent_x, tangent_y, linestyle='--', label=f'Slope at x={x0:.1f}: {slope:.1f}')\n",
        "    plt.scatter(x0, y0, color='red')\n",
        "\n",
        "    # Annotate slope direction\n",
        "    direction = '↓ (negative)' if slope < 0 else ('↑ (positive)' if slope > 0 else '→ (zero)')\n",
        "    plt.annotate(f\"Slope = {slope:.1f}\\n{direction}\", (x0, y0 + 0.5), fontsize=10, ha='center')\n",
        "\n",
        "# Plot settings\n",
        "plt.title(\"Visualizing Slope of $f(x) = (x - 3)^2$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NyXNMLhqm0No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simple data\n",
        "x = np.array([1, 2, 3])\n",
        "y = np.array([2, 4, 6])\n",
        "\n",
        "# Mean Squared Error Loss\n",
        "def compute_loss(w):\n",
        "    y_pred = w * x\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "# Try various w values\n",
        "w_values = np.linspace(0, 4, 100)\n",
        "loss_values = [compute_loss(w) for w in w_values]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(w_values, loss_values, label=\"Loss vs Weight\", color='purple')\n",
        "plt.axvline(x=2, linestyle='--', color='green', label='Minimum Loss at w=2')\n",
        "plt.scatter([2], [compute_loss(2)], color='red', zorder=5)\n",
        "plt.title(\"Mapping Model Parameter (w) to Error (Loss)\")\n",
        "plt.xlabel(\"Weight (w)\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-AbnaR_GnAk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 6, 8])\n",
        "\n",
        "model = SGDRegressor(max_iter = 1000, learning_rate = 'constant', eta0=0.01)\n",
        "model.fit(x,y)\n",
        "\n",
        "print(\"Coefficient:\", model.coef_[0])"
      ],
      "metadata": {
        "id": "Bu4OFXpGnLK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Lasso, Ridge, SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Simple dataset\n",
        "x = np.array([[1], [2], [3], [4], [5]])\n",
        "y = np.array([1.2, 2.4, 3.1, 4.3, 5.1])\n",
        "\n",
        "# Lasso Regression\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(x, y)\n",
        "print(\"Lasso Coefficient:\", lasso.coef_[0])\n",
        "\n",
        "# Ridge Regression\n",
        "ridge = Ridge(alpha=0.1)\n",
        "ridge.fit(x, y)\n",
        "print(\"Ridge Coefficient:\", ridge.coef_[0])\n",
        "\n",
        "# SGDRegressor\n",
        "sgd = SGDRegressor(penalty='l2', alpha=0.1, max_iter=1000, learning_rate='constant', eta0=0.01)\n",
        "sgd.fit(x, y)\n",
        "print(\"SGD Coefficient:\", sgd.coef_[0])"
      ],
      "metadata": {
        "id": "whthw5gVoYRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3TTQnxDnak_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write me a code for page rank. use a simple data set of 3 to 4 pages. show me the page ranks of each iteration. don't approximate the rank values.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def page_rank(adj_matrix, damping_factor=0.85, iterations=100):\n",
        "  \"\"\"\n",
        "  Calculates PageRank for a given adjacency matrix.\n",
        "\n",
        "  Args:\n",
        "    adj_matrix: A NumPy array representing the adjacency matrix of the web graph.\n",
        "    damping_factor: The damping factor (probability of following a link).\n",
        "    iterations: The number of iterations to perform.\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array containing the PageRank values for each page.\n",
        "  \"\"\"\n",
        "  num_pages = len(adj_matrix)\n",
        "  page_ranks = np.ones(num_pages) / num_pages  # Initialize PageRank values\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    print(f\"Iteration {iteration + 1}:\")\n",
        "    new_page_ranks = np.zeros(num_pages)\n",
        "    for i in range(num_pages):\n",
        "        for j in range(num_pages):\n",
        "            if adj_matrix[j][i] == 1:\n",
        "                out_degree = np.sum(adj_matrix[j])\n",
        "                if out_degree > 0 :\n",
        "                    new_page_ranks[i] += damping_factor * (page_ranks[j] / out_degree)\n",
        "                else:\n",
        "                    new_page_ranks[i] += damping_factor * (page_ranks[j] / num_pages)\n",
        "\n",
        "        new_page_ranks[i] += (1 - damping_factor) / num_pages\n",
        "\n",
        "    page_ranks = new_page_ranks\n",
        "    print(page_ranks)\n",
        "\n",
        "  return page_ranks\n",
        "\n",
        "# Example usage:\n",
        "# Adjacency matrix for a simple web graph with 4 pages\n",
        "# Page 1 links to pages 2 and 3\n",
        "# Page 2 links to page 1\n",
        "# Page 3 links to page 4\n",
        "# Page 4 links to page 3\n",
        "\n",
        "adjacency_matrix = np.array([\n",
        "    [0, 1, 1, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 1, 0]\n",
        "])\n",
        "\n",
        "final_ranks = page_rank(adjacency_matrix)\n",
        "\n",
        "print(\"\\nFinal PageRanks:\", final_ranks)\n"
      ],
      "metadata": {
        "id": "fc9_l4vBuJiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first Implementation of PageRank\n",
        "# !pip install python-igraph\n",
        "from igraph import Graph\n",
        "\n",
        "g = Graph(directed=True)\n",
        "g.add_vertices(3)\n",
        "g.add_edges([(0,1), (0,2), (1,2), (2,0)])\n",
        "pr = g.pagerank()\n",
        "print(pr)"
      ],
      "metadata": {
        "id": "EAnFSvryKUDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter Followers\n",
        "# Install igraph if not already installed\n",
        "#!pip install python-igraph\n",
        "\n",
        "from igraph import Graph\n",
        "from igraph import plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a directed graph (Twitter-style follow graph)\n",
        "# Each directed edge (A, B) means \"A follows B\"\n",
        "users = [\"Deepanshu\", \"Zoya\", \"Tinku\", \"Aryan\", \"Laxman\"]\n",
        "edges = [\n",
        "    (\"Deepanshu\", \"Zoya\"),\n",
        "    (\"Deepanshu\", \"Tinku\"),\n",
        "    (\"Deepanshu\", \"Aryan\"), (\"Aryan\", \"Deepanshu\"),\n",
        "    (\"Zoya\", \"Tinku\"),\n",
        "    (\"Tinku\", \"Aryan\"),\n",
        "    (\"Aryan\", \"Laxman\")]\n",
        "\n",
        "# Step 2: Build the igraph Graph\n",
        "g = Graph(directed=True)\n",
        "g.add_vertices(users)\n",
        "g.add_edges(edges)\n",
        "\n",
        "# Step 3: Compute PageRank (higher = more influential)\n",
        "page_ranks = g.pagerank(damping = 0.85)\n",
        "\n",
        "# Step 4: Display results\n",
        "print(\"Influence Scores (PageRank):\")\n",
        "for user, score in zip(users, page_ranks):\n",
        "    print(f\"{user}: {score:.4f}\")\n",
        "\n",
        "# Step 5: Visualize the graph\n",
        "layout = g.layout(\"fr\")  # Force-directed layout\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "g.vs[\"label\"] = users\n",
        "g.vs[\"size\"] = [20 + 100 * pr for pr in page_ranks]  # Size nodes by rank\n",
        "g.vs[\"color\"] = \"skyblue\"\n",
        "g.es[\"arrow_size\"] = 0.5\n",
        "plot(g, layout=layout, vertex_label=g.vs[\"label\"], target=ax)\n",
        "plt.title(\"Twitter-style Follower Network with PageRank\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zw0SDaEqKgfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zachary's Karate Club - Relationship Graph\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the built-in Karate Club graph from NetworkX\n",
        "G = nx.karate_club_graph()\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, seed=42)  # For consistent layout\n",
        "\n",
        "# Draw nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', edgecolors='black')\n",
        "\n",
        "# Draw edges (relationships)\n",
        "nx.draw_networkx_edges(G, pos, width=2)\n",
        "\n",
        "# Draw node labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=12, font_color='black')\n",
        "\n",
        "# Title and display\n",
        "plt.title(\"Zachary's Karate Club - Relationship Graph\", fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2e2XEzVNMzQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the graph\n",
        "G = nx.karate_club_graph()\n",
        "\n",
        "# Parameters\n",
        "damping_factor = 0.85\n",
        "max_iter = 100\n",
        "tol = 1e-6\n",
        "\n",
        "# Initialize\n",
        "N = G.number_of_nodes()\n",
        "pr = np.ones(N) / N  # Initial PageRank values\n",
        "adj = nx.to_numpy_array(G)  # Adjacency matrix\n",
        "\n",
        "# Normalize the adjacency matrix\n",
        "out_degree = adj.sum(axis=1)\n",
        "transition_matrix = adj / out_degree[:, None]\n",
        "\n",
        "# PageRank iteration\n",
        "history = []\n",
        "for i in range(max_iter):\n",
        "    new_pr = (1 - damping_factor) / N + damping_factor * transition_matrix.T @ pr\n",
        "    delta = np.abs(new_pr - pr).sum()\n",
        "    history.append((i + 1, new_pr.copy(), delta))\n",
        "    pr = new_pr\n",
        "    if delta < tol:\n",
        "        print(f\"Converged at iteration {i+1} with total change {delta:.6f}\")\n",
        "        break\n",
        "\n",
        "# Display results\n",
        "print(\"\\nIteration-wise PageRank Scores:\")\n",
        "for i, scores, delta in history:\n",
        "    print(f\"Iter {i:2d} | Δ = {delta:.6f} | Top 3 nodes: {np.argsort(scores)[-3:][::-1]}\")\n",
        "\n",
        "# Plot convergence\n",
        "deltas = [h[2] for h in history]\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(deltas)+1), deltas, marker='o')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Total Change (Δ)')\n",
        "plt.title('Convergence of PageRank on Karate Club Graph')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OD6y3wYtOvUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Load the Karate Club dataset (built into NetworkX)\n",
        "G = nx.karate_club_graph()\n",
        "\n",
        "# Compute the PageRank\n",
        "pagerank = nx.pagerank(G, alpha=0.85)  # alpha is the damping factor (default is 0.85)\n",
        "\n",
        "# Display the nodes with the highest PageRank scores\n",
        "sorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
        "top_n = sorted_pagerank[:34]\n",
        "print(\"All nodes by PageRank:\")\n",
        "for node, score in top_n:\n",
        "    print(f\"Node {node}: {score}\")\n",
        "\n",
        "# Optional: Visualize the graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(G, with_labels=True, node_size=500, font_size=10, font_color='red', node_color='pink')\n",
        "plt.title(\"Karate Club Graph with PageRank\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-xbX6wDPbVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Create a directed graph\n",
        "G =\n",
        "\n",
        "# Step 2: Add endorsements (A -> B means A endorsed B)\n",
        "endorsements = [\n",
        "\n",
        "]\n",
        "\n",
        "G.add_edges_from()\n",
        "\n",
        "# Step 3: Compute PageRank\n",
        "\n",
        "\n",
        "# Step 4: Display results (without f-strings)\n",
        "print(\"LinkedIn-style Endorsement PageRank Scores:\")\n",
        "for user, score in pageranks.items():\n",
        "    print(user + \": \" + str(round(score, 4)))\n",
        "\n",
        "# Step 5: Visualize the graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "pos = nx.spring_layout(G)\n",
        "node_sizes = [5000 * pageranks[node] for node in G.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"lightgreen\")\n",
        "nx.draw_networkx_edges(G, pos, arrows=True)\n",
        "nx.draw_networkx_labels(G, pos)\n",
        "\n",
        "plt.title(\"LinkedIn Endorsement Network (Node size = Influence)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyb9iE9uQKeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The curious Case - What do you take away?\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create the \"failing\" graph: A → B → C → D\n",
        "G = nx.DiGraph()\n",
        "edges = [('A', 'B'), ('B', 'C'), ('C', 'D')]\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Run PageRank\n",
        "pagerank = nx.pagerank(G, alpha=0.85)\n",
        "\n",
        "# Show PageRank values\n",
        "print(\"PageRank Scores:\")\n",
        "for node, score in pagerank.items():\n",
        "    print(f\"{node}: {score:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 4))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Draw nodes with PageRank-based size\n",
        "node_sizes = [pagerank[n] * 5000 for n in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color='lightcoral', arrows=True, edge_color='gray')\n",
        "\n",
        "# Annotate PageRank values\n",
        "for node, (x, y) in pos.items():\n",
        "    plt.text(x, y + 0.08, f\"{pagerank[node]:.4f}\", ha='center', fontsize=10, color='black')\n",
        "\n",
        "plt.title(\" PageRank Behavior on a Dangling Chain (A → B → C → D)\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pmvcTEhHRmRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The story of Apples and Oranges\n",
        "# Where are we heading at?"
      ],
      "metadata": {
        "id": "28A7C13A70Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does that look like in code?\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Red and orange points\n",
        "red_points = np.array([\n",
        "    [2, 4], [3, 3], [4, 5], [3, 5], [2.5, 4], [3.2, 4.8], [4.1, 4.5], [2.8, 3.7]\n",
        "])\n",
        "orange_points = np.array([\n",
        "    [6, 1], [7, 2], [8, 2], [7, 1], [6.5, 2], [7.5, 1.8], [8.1, 1.9], [6.8, 1.5]\n",
        "])\n",
        "\n",
        "# Combine data\n",
        "X = np.vstack((red_points, orange_points))\n",
        "y = np.array([0]*len(red_points) + [1]*len(orange_points))\n",
        "\n",
        "# Train linear SVM\n",
        "model = SVC(kernel='linear', C=1)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Extract model parameters\n",
        "w = model.coef_[0]\n",
        "b = model.intercept_[0]\n",
        "slope = -w[0] / w[1]\n",
        "intercept = -b / w[1]\n",
        "\n",
        "# Create a grid to draw hyperplane and margins\n",
        "xx = np.linspace(1, 9, 100)\n",
        "yy = slope * xx + intercept\n",
        "\n",
        "# Margin calculation\n",
        "margin = 1 / np.sqrt(np.sum(w ** 2))\n",
        "yy_down = yy - np.sqrt(1 + slope**2) * margin\n",
        "yy_up = yy + np.sqrt(1 + slope**2) * margin\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(red_points[:, 0], red_points[:, 1], color='red')\n",
        "plt.scatter(orange_points[:, 0], orange_points[:, 1], color='orange')\n",
        "plt.plot(xx, yy, 'k-')\n",
        "plt.plot(xx, yy_down, 'k--')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.title('The apples and oranges')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iSjCgqMj77tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code with Labels\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# More red and orange points\n",
        "red_points = np.array([\n",
        "    [2, 4], [3, 3], [4, 5], [3, 5], [2.5, 4], [3.2, 4.8], [4.1, 4.5], [2.8, 3.7]\n",
        "])\n",
        "orange_points = np.array([\n",
        "    [6, 1], [7, 2], [8, 2], [7, 1], [6.5, 2], [7.5, 1.8], [8.1, 1.9], [6.8, 1.5]\n",
        "])\n",
        "\n",
        "# Combine data\n",
        "X = np.vstack((red_points, orange_points))\n",
        "y = np.array([0]*len(red_points) + [1]*len(orange_points))\n",
        "\n",
        "# Train linear SVM\n",
        "model = SVC(kernel='linear', C=1)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Extract model parameters\n",
        "w = model.coef_[0]\n",
        "b = model.intercept_[0]\n",
        "slope = -w[0] / w[1]\n",
        "intercept = -b / w[1]\n",
        "\n",
        "# Create a grid to draw hyperplane and margins\n",
        "xx = np.linspace(1, 9, 100)\n",
        "yy = slope * xx + intercept\n",
        "\n",
        "# Margin calculation\n",
        "margin = 1 / np.sqrt(np.sum(w ** 2))\n",
        "yy_down = yy - np.sqrt(1 + slope**2) * margin\n",
        "yy_up = yy + np.sqrt(1 + slope**2) * margin\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(red_points[:, 0], red_points[:, 1], color='red', label='Apples')\n",
        "plt.scatter(orange_points[:, 0], orange_points[:, 1], color='orange', label='Oranges')\n",
        "plt.plot(xx, yy, 'k-', label='Hyperplane')\n",
        "plt.plot(xx, yy_down, 'k--', label='Margin')\n",
        "plt.plot(xx, yy_up, 'k--')\n",
        "plt.legend()\n",
        "plt.title('SVM: Hyperplane and Margins')\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dtEGDS2c8Vge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load a small dataset with only two features\n",
        "X, y = datasets.make_blobs(n_samples=20, centers=2, random_state=6)\n",
        "\n",
        "# Train a linear SVM\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')\n",
        "\n",
        "# Plot the decision boundary (hyperplane)\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# Create a grid to evaluate the model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "Z = model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# Plot decision boundary and margins\n",
        "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], linestyles=['--', '-', '--'])\n",
        "\n",
        "# Plot support vectors\n",
        "ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "           s=100, linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')\n",
        "\n",
        "plt.title(\"Simple SVM Demo\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LGaz8Qs1-urz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "# Generate non-linear sample data (circles)\n",
        "X, y = datasets.make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)\n",
        "\n",
        "# Plot the raw data only\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')\n",
        "plt.title(\"Sample Circular Data\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WdopCQjUBFUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What if we visualize it in multiple dimensions?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Generate the same circular data\n",
        "X, y = make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply a manual transformation to lift the data into 3D\n",
        "# Let's use: z = x^2 + y^2 (a common feature map for circles)\n",
        "x1 = X[:, 0]\n",
        "x2 = X[:, 1]\n",
        "z = x1**2 + x2**2\n",
        "\n",
        "# Plot in 3D\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Color map: red (0) and orange (1)\n",
        "colors = np.where(y == 0, 'red', 'orange')\n",
        "\n",
        "ax.scatter(x1, x2, z, c=colors, edgecolor='k')\n",
        "ax.set_xlabel('X1')\n",
        "ax.set_ylabel('X2')\n",
        "ax.set_zlabel('Z = X1² + X2²')\n",
        "ax.set_title('Data Transformed to 3D (Z = X1² + X2²)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_wT4A4smBZuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding the 4 kernels\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import svm, datasets\n",
        "\n",
        "# Generate non-linear sample data\n",
        "X, y = datasets.make_circles(n_samples=200, factor=0.3, noise=0.05, random_state=42)\n",
        "\n",
        "# Set up the SVM classifiers with different kernels\n",
        "kernels = {\n",
        "    'Linear': svm.SVC(kernel='linear'),\n",
        "    'Polynomial': svm.SVC(kernel='poly', degree = 5),\n",
        "    'Radial Basis': svm.SVC(kernel='rbf'),\n",
        "    'Sigmoid': svm.SVC(kernel='sigmoid')\n",
        "}\n",
        "\n",
        "# Train all models\n",
        "models = {}\n",
        "for name, clf in kernels.items():\n",
        "    clf.fit(X, y)\n",
        "    models[name] = clf\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i, (name, model) in enumerate(models.items(), 1):\n",
        "    plt.subplot(2, 2, i)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 500),\n",
        "                         np.linspace(-1.5, 1.5, 500))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 20), cmap='autumn', alpha=0.5)\n",
        "    plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='lightblue', alpha=0.5)\n",
        "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='k')  # decision boundary\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='k')\n",
        "    plt.title(name)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"SVM Decision Boundaries with Different Kernels\", fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4MbKL1gCD7El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with IRIS data set\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only first two features for easy 2D visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM\n",
        "model = SVC(kernel='rbf', gamma='auto')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot decision boundaries\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sepal length (standardized)')\n",
        "    plt.ylabel('Sepal width (standardized)')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_train, y_train, model, \"SVM with RBF Kernel (Iris Dataset)\")\n"
      ],
      "metadata": {
        "id": "5RX_BkaMLVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with penguin data set\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from matplotlib.cbook import get_sample_data\n",
        "import seaborn as sns\n",
        "\n",
        "# Load penguin dataset\n",
        "penguins = sns.load_dataset(\"penguins\")\n",
        "\n",
        "# Drop rows with missing values\n",
        "penguins.dropna(inplace=True)\n",
        "\n",
        "# Select features and labels\n",
        "X = penguins[['bill_length_mm', 'flipper_length_mm']].values\n",
        "y = penguins['species'].values\n",
        "\n",
        "# Encode species labels\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model\n",
        "model = SVC(kernel='rbf', gamma='scale')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    handles, _ = scatter.legend_elements()\n",
        "    plt.legend(handles=list(handles), labels=list(le.classes_))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Bill Length (standardized)\")\n",
        "    plt.ylabel(\"Flipper Length (standardized)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting\n",
        "plot_decision_boundary(X_train, y_train, model, \"SVM on Penguin Dataset (RBF Kernel)\")\n"
      ],
      "metadata": {
        "id": "viD0zdxdXkFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset and filter for only 2 species\n",
        "penguins = sns.load_dataset(\"penguins\").dropna()\n",
        "penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]\n",
        "\n",
        "# Features and target\n",
        "X = penguins[['bill_length_mm', 'flipper_length_mm']].values\n",
        "y = penguins['species'].values\n",
        "\n",
        "# Encode target labels (Adelie=0, Gentoo=1)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "model = SVC(kernel='rbf', gamma='scale')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plotting decision boundary with support vectors\n",
        "def plot_decision_boundary(X, y, model, title):\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "    # Highlight the support vectors\n",
        "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "                facecolors='none', edgecolors='k', s=100, linewidths=1.5, label=\"Support Vectors\")\n",
        "\n",
        "    # Separate legends: one for the data points and one for the support vectors\n",
        "    handles, labels = scatter.legend_elements()\n",
        "    plt.legend(handles=handles, labels=list(le.classes_))\n",
        "    plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='k', markersize=10, label=\"Support Vectors\")],\n",
        "               loc='upper right')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Bill Length\")\n",
        "    plt.ylabel(\"Flipper Length\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary with support vectors\n",
        "plot_decision_boundary(X_train, y_train, model, \"SVM with RBF Kernel (Adelie vs Gentoo)\")\n"
      ],
      "metadata": {
        "id": "uMZ-c0ckXtN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load and clean penguins dataset\n",
        "penguins = sns.load_dataset('penguins')\n",
        "penguins = penguins.dropna()\n",
        "penguins = penguins[penguins['species'].isin(['Adelie', 'Gentoo'])]\n",
        "\n",
        "# Features and labels\n",
        "X = penguins[['bill_length_mm', 'flipper_length_mm']].values\n",
        "y = penguins['species'].values\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)  # 0 = Adelie, 1 = Gentoo\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plotting function with margins\n",
        "def plot_svm_with_margins(X, y, model, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Create mesh grid\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Compute decision function over mesh\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary and margins\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    contour = plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'],\n",
        "                          colors='k', linewidths=1.5)\n",
        "\n",
        "    # Plot data points\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "\n",
        "    # Plot support vectors\n",
        "    sv = model.support_vectors_\n",
        "    plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none',\n",
        "                edgecolors='black', linewidth=1.5, label='Support Vectors')\n",
        "\n",
        "    plt.xlabel('Bill Length (scaled)')\n",
        "    plt.ylabel('Flipper Length (scaled)')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_svm_with_margins(X_train, y_train, model, \"SVM with Decision Boundary, Margins & Support Vectors\")\n"
      ],
      "metadata": {
        "id": "UQ2sqjBxX4TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Data on Purchase Pattern\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('batch-data.csv')\n",
        "X = df[['age', 'cost']].values\n",
        "y = df['purchase'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM model with RBF kernel\n",
        "model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Plot with decision boundary, margins, and support vectors\n",
        "def plot_svm_margins(X, y, model, title):\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1\n",
        "    y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')\n",
        "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Age (scaled)')\n",
        "    plt.ylabel('Cost (scaled)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_svm_margins(X_train_scaled, y_train, model, \"SVM with RBF Kernel\\n(Support Vectors & Margins)\")\n",
        "\n",
        "# Classification report\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NZRyJZ7XmFQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Purchase Pattern and Prediction\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('batch-data.csv')\n",
        "X = df[['age', 'cost']].values\n",
        "y = df['purchase'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# SVM model with RBF kernel\n",
        "model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# New data point to predict\n",
        "new_tuple = np.array([[23,4000]])\n",
        "new_tuple_scaled = scaler.transform(new_tuple)\n",
        "predicted_class = model.predict(new_tuple_scaled)[0]\n",
        "\n",
        "# Plot function with decision boundary, margins, support vectors, and new point\n",
        "def plot_svm_margins_with_new_point(X, y, model, new_point_scaled, title):\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min()-1, X[:, 0].max()+1\n",
        "    y_min, y_max = X[:, 1].min()-1, X[:, 1].max()+1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')\n",
        "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k', label='Train Data')\n",
        "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')\n",
        "\n",
        "    # New point marker\n",
        "    plt.scatter(new_point_scaled[0][0], new_point_scaled[0][1],\n",
        "                color='green', marker='X', s=150, label='New Tuple')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Age (scaled)')\n",
        "    plt.ylabel('Cost (scaled)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot\n",
        "plot_svm_margins_with_new_point(X_train_scaled, y_train, model, new_tuple_scaled,\n",
        "                                \"SVM with RBF Kernel\\n(Support Vectors, Margins, and New Tuple)\")\n",
        "\n",
        "# Classification report\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "print(f\"\\nPrediction: {predicted_class} (0 = Not Purchase, 1 = Purchase)\")\n"
      ],
      "metadata": {
        "id": "gElX-46cm_sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('classification_results.csv')\n",
        "y_true = df['actual']\n",
        "y_pred = df['predicted']\n",
        "\n",
        "# Confusion matrix and metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "NeKHtQqdut9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Each Term\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "print(\"Accuracy\", accuracy)\n",
        "print(\"Precision\", precision)\n",
        "print(\"Recall\", recall)\n",
        "print(\"F1 Score\", f1)"
      ],
      "metadata": {
        "id": "2TS9j96tvZjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bank Data Set from UCI\n",
        "\n",
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Banknote dataset from UCI\n",
        "df = pd.read_csv('data_banknote_authentication.txt')\n",
        "df.columns = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
        "\n",
        "# We'll use only two features for visualization\n",
        "X = df[['variance', 'skewness']].values\n",
        "y = df['class'].values\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM with RBF kernel\n",
        "model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Plotting decision boundaries, margins, and support vectors\n",
        "def plot_svm_margins(X, y, model, title):\n",
        "    h = 0.01\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z > 0, alpha=0.3, cmap='coolwarm')\n",
        "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], colors='k')\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
        "                s=100, facecolors='none', edgecolors='blue', label='Support Vectors')\n",
        "\n",
        "    plt.xlabel('Variance (scaled)')\n",
        "    plt.ylabel('Skewness (scaled)')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize\n",
        "plot_svm_margins(X_train_scaled, y_train, model, \"SVM with RBF Kernel on Banknote Data\")\n"
      ],
      "metadata": {
        "id": "k23cN3tzyktF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the updated bank loan dataset\n",
        "# Load CSV\n",
        "df = pd.read_csv('/content/bank_loan_data.csv')  # Update path if needed\n",
        "\n",
        "# Feature selection and target\n",
        "X = df[['Annual_Income', 'Credit_Score']].values\n",
        "y = df['Loan_Approved'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter ranges\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "gamma_values = [0.01, 0.1, 1]\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for gamma in gamma_values:\n",
        "    for C in C_values:\n",
        "        model = SVC(kernel='rbf', C=C, gamma=gamma)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        results.append({\n",
        "            'C': C,\n",
        "            'gamma': gamma,\n",
        "            'Accuracy': acc,\n",
        "            'Precision': prec,\n",
        "            'Recall': rec,\n",
        "            'F1 Score': f1\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame for display\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['C', 'gamma', 'Accuracy', 'Precision', 'Recall', 'F1 Score']]\n",
        "results_df.sort_values(by=['gamma', 'C'], inplace=True)\n",
        "results_df.reset_index(drop=True, inplace=True)\n",
        "results_df.round(3)"
      ],
      "metadata": {
        "id": "B9gBCzOZ9KGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Revision\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# 1. Create the dataset\n",
        "data = {\n",
        "    'individual_study_hours': [1, 2, 3, 4, 5, 6],\n",
        "    'group_study_hours': [3, 3, 3, 3, 4, 4],\n",
        "    'marks_scored': [56, 60, 65, 66, 75, 87]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# 2. Define X and y\n",
        "X = df[['individual_study_hours', 'group_study_hours']]\n",
        "y = df['marks_scored']\n",
        "\n",
        "# 3. Fit Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# 4. Coefficients and intercept\n",
        "print(\"\\nCoefficients:\", lr.coef_)\n",
        "print(\"Intercept:\", lr.intercept_)"
      ],
      "metadata": {
        "id": "lEU3b7VJEdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Revision\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Create the dataset\n",
        "data = {\n",
        "    'individual_study_hours': [1,2,3,4,5,6],\n",
        "    'group_study_hours': [3,3,3,3,4,4],\n",
        "    'marks_scored': [56, 60, 65, 66, 75, 87]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# 2. Define X and y\n",
        "X = df[['individual_study_hours', 'group_study_hours']]\n",
        "y = df['marks_scored']\n",
        "\n",
        "# 3. Fit Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# 4. Coefficients and intercept\n",
        "print(\"\\nCoefficients:\", lr.coef_)\n",
        "print(\"Intercept:\", lr.intercept_)\n",
        "\n",
        "# 5. Predict and compute R²\n",
        "y_pred = lr.predict(X)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(\"\\nR2 Score:\", r2 )\n",
        "\n",
        "# 6. Show predictions\n",
        "print(\"\\nPredictions:\", y_pred)"
      ],
      "metadata": {
        "id": "Q1UMHTigF2W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "\n",
        "coefs = lr.coef_\n",
        "alpha = 1\n",
        "\n",
        "# Lasso Penalty with alpha\n",
        "lasso_penalty = alpha * np.sum(np.abs(coefs))\n",
        "\n",
        "# Ridge Penalty with alpha\n",
        "ridge_penalty = alpha * np.sum(coefs**2)\n",
        "\n",
        "print(\"Lasso Penalty (with alpha=1):\", lasso_penalty)\n",
        "print(\"Ridge Penalty (with alpha=1):\", ridge_penalty)\n",
        "\n"
      ],
      "metadata": {
        "id": "ztdTiiSqIRRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regularization Revision\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Create the dataset\n",
        "data = {\n",
        "    'individual_study_hours': [1,2,3,4,5,6],\n",
        "    'group_study_hours': [3,3,3,3,4,4],\n",
        "    'marks_scored': [56, 60, 65, 66, 75, 87]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# 2. Define X and y\n",
        "X = df[['individual_study_hours', 'group_study_hours']]\n",
        "y = df['marks_scored']\n",
        "\n",
        "# 3. Fit Linear Regression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# 4. Coefficients and intercept for Linear Regression\n",
        "print(\"\\nLinear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Linear Regression Intercept:\", lr.intercept_)\n",
        "\n",
        "# 5. Fit Lasso Regression\n",
        "lasso = Lasso(alpha=1)\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# 6. Coefficients and intercept for Lasso Regression\n",
        "print(\"\\nLasso Regression Coefficients:\", lasso.coef_)\n",
        "print(\"Lasso Regression Intercept:\", lasso.intercept_)\n",
        "\n",
        "# 7. Fit Ridge Regression\n",
        "ridge = Ridge(alpha=1)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "# 8. Coefficients and intercept for Ridge Regression\n",
        "print(\"\\nRidge Regression Coefficients:\", ridge.coef_)\n",
        "print(\"Ridge Regression Intercept:\", ridge.intercept_)\n",
        "\n",
        "# 9. Make predictions with the models\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "\n",
        "# 10. Compute R² for each model\n",
        "r2_lr = r2_score(y, y_pred_lr)\n",
        "r2_lasso = r2_score(y, y_pred_lasso)\n",
        "r2_ridge = r2_score(y, y_pred_ridge)\n",
        "\n",
        "print(\"\\nR² Score for Linear Regression:\", r2_lr)\n",
        "print(\"R² Score for Lasso Regression:\", r2_lasso)\n",
        "print(\"R² Score for Ridge Regression:\", r2_ridge)\n"
      ],
      "metadata": {
        "id": "YIQ0EoCMEf9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Create the dataset\n",
        "data = {\n",
        "    'individual_study_hours': [1,2,3,4,5,6],\n",
        "    'group_study_hours': [3,3,3,3,4,4],\n",
        "    'marks_scored': [56, 60, 65, 66, 75, 87]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# 2. Define X and y\n",
        "X = df[['individual_study_hours', 'group_study_hours']]\n",
        "y = df['marks_scored']\n",
        "\n",
        "# 3. Fit Linear Regression (No Regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# 4. Coefficients and intercept for Linear Regression\n",
        "print(\"\\nLinear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Linear Regression Intercept:\", lr.intercept_)\n",
        "\n",
        "# 5. Make a prediction for a new data point\n",
        "new_data_point = pd.DataFrame([[7,3]], columns=['individual_study_hours', 'group_study_hours'])\n",
        "\n",
        "# Predict the score for the new data point\n",
        "predicted_score = lr.predict(new_data_point)\n",
        "\n",
        "# 6. Print the predicted score\n",
        "print(\"\\nPredicted Marks:\", predicted_score[0])\n",
        "\n",
        "# 7. Compute R² to evaluate the model's performance\n",
        "y_pred = lr.predict(X)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(\"\\nR² Score:\", r2)\n"
      ],
      "metadata": {
        "id": "zvQWYm8yNSFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Create the dataset\n",
        "data = {\n",
        "    'individual_study_hours': [1,2,3,4,5,6],\n",
        "    'group_study_hours': [3,3,3,3,4,4],\n",
        "    'marks_scored': [56, 60, 65, 66, 75, 87]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\n",
        "# 2. Define X and y\n",
        "X = df[['individual_study_hours', 'group_study_hours']]\n",
        "y = df['marks_scored']\n",
        "\n",
        "# 3. Fit Linear Regression (No Regularization)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# 4. Coefficients and intercept for Linear Regression\n",
        "print(\"\\nLinear Regression Coefficients:\", lr.coef_)\n",
        "print(\"Linear Regression Intercept:\", lr.intercept_)\n",
        "\n",
        "# 5. Fit Lasso Regression\n",
        "lasso = Lasso()\n",
        "lasso.fit(X, y)\n",
        "print(\"\\nLasso Regression Coefficients:\", lasso.coef_)\n",
        "print(\"Lasso Regression Intercept:\", lasso.intercept_)\n",
        "\n",
        "# 6. Fit Ridge Regression\n",
        "ridge = Ridge()\n",
        "ridge.fit(X, y)\n",
        "print(\"\\nRidge Regression Coefficients:\", ridge.coef_)\n",
        "print(\"Ridge Regression Intercept:\", ridge.intercept_)\n",
        "\n",
        "# 7. Make a prediction for a new data point\n",
        "new_data_point = pd.DataFrame([[7,3]], columns=['individual_study_hours', 'group_study_hours'])\n",
        "\n",
        "# Predict the score for the new data point using Linear Regression\n",
        "lr_pred = lr.predict(new_data_point)\n",
        "# Predict the score for the new data point using Lasso\n",
        "lasso_pred = lasso.predict(new_data_point)\n",
        "# Predict the score for the new data point using Ridge\n",
        "ridge_pred = ridge.predict(new_data_point)\n",
        "\n",
        "# 8. Print the predicted scores for the new data point\n",
        "print(\"\\nPredicted Marks:\")\n",
        "print(f\"Linear Regression Prediction: {lr_pred[0]}\")\n",
        "print(f\"Lasso Regression Prediction: {lasso_pred[0]}\")\n",
        "print(f\"Ridge Regression Prediction: {ridge_pred[0]}\")\n",
        "\n",
        "# 9. Compute R² for each model\n",
        "lr_r2 = r2_score(y, lr.predict(X))\n",
        "lasso_r2 = r2_score(y, lasso.predict(X))\n",
        "ridge_r2 = r2_score(y, ridge.predict(X))\n",
        "\n",
        "print(\"\\nR² Score for Linear Regression:\", lr_r2)\n",
        "print(\"R² Score for Lasso Regression:\", lasso_r2)\n",
        "print(\"R² Score for Ridge Regression:\", ridge_r2)\n"
      ],
      "metadata": {
        "id": "GAR7a320N3pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# === Step 1: Create Dataset ===\n",
        "X = np.array([\n",
        "    [1, 1],\n",
        "    [2, 2],\n",
        "    [3, 3],\n",
        "    [4, 4],\n",
        "    [5, 5],\n",
        "    [6, 6]\n",
        "])\n",
        "y = np.array([3, 5, 7, 9, 11, 13])  # Simple linear target: y = x1 + x2 + 1\n",
        "\n",
        "# === Step 2: Train Models ===\n",
        "lr = LinearRegression().fit(X, y)\n",
        "ridge = Ridge(alpha=1.0).fit(X, y)\n",
        "lasso = Lasso(alpha=1.0).fit(X, y)\n",
        "\n",
        "# === Step 3: Make Predictions on Training Data ===\n",
        "y_pred_lr = lr.predict(X)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "\n",
        "# === Step 4: Show Coefficients, MSE, and R² Scores ===\n",
        "print(\"=== Linear Regression ===\")\n",
        "print(\"Coefficients:\", lr.coef_)\n",
        "print(\"Intercept:\", lr.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lr))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lr))\n",
        "\n",
        "print(\"\\n=== Ridge Regression ===\")\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "print(\"Intercept:\", ridge.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_ridge))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_ridge))\n",
        "\n",
        "print(\"\\n=== Lasso Regression ===\")\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "print(\"Intercept:\", lasso.intercept_)\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lasso))\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lasso))\n",
        "\n",
        "# === Step 5: Plot the Predictions ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y, label='True Values', marker='o', linewidth=2)\n",
        "plt.plot(y_pred_lr, label='Linear Regression', marker='s')\n",
        "plt.plot(y_pred_ridge, label='Ridge (α=1.0)', marker='^')\n",
        "plt.plot(y_pred_lasso, label='Lasso (α=1.0)', marker='x')\n",
        "plt.title(\"Predictions: Linear vs Ridge vs Lasso\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Predicted Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === Step 6: Predict on New Input ===\n",
        "new_sample = np.array([[7,7]])\n",
        "\n",
        "pred_lr = lr.predict(new_sample)\n",
        "pred_ridge = ridge.predict(new_sample)\n",
        "pred_lasso = lasso.predict(new_sample)\n",
        "\n",
        "print(\"\\n=== Prediction for New Input ===\")\n",
        "print(\"Linear Regression Prediction:\", pred_lr[0])\n",
        "print(\"Ridge Regression Prediction:\", pred_ridge[0])\n",
        "print(\"Lasso Regression Prediction:\", pred_lasso[0])\n"
      ],
      "metadata": {
        "id": "kxXWNgrASf5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------Neurons------------------------\n",
        "#               - the story\n",
        "#--------------------------------------------------\n",
        "\n",
        "# Inputs are about: Ear, Whisker and Fur\n",
        "inputs = [1, 0, 1]\n",
        "weights = [0.5, 0.7, 0.6]\n",
        "threshold = 1.2\n",
        "\n",
        "weighted_sum = (inputs[0] * weights[0]) + (inputs[1] * weights[1]) + (inputs[2] * weights[2])\n",
        "\n",
        "if weighted_sum >= threshold:\n",
        "  print(\"Neurons fired: Cat\")\n",
        "else:\n",
        "  print(\"Not a cat\")\n"
      ],
      "metadata": {
        "id": "lyuNB2Ek6ddr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Session: Perceptrons**\n",
        "\n",
        "Perceptron is a single-layer neural network linear or a Machine Learning algorithm used for supervised learning of various binary classifiers.\n",
        "\n",
        "**Sheet Link:**\n",
        "https://docs.google.com/spreadsheets/d/1KcfcvPQ40HLCK85LeahIZboLufVGVl0mpSxshLISM00/edit?gid=0#gid=0\n",
        "\n",
        "Perceptron is a linear model. A perceptron learns to separate data using a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions). It can only solve linearly separable problems — that is, problems where the data classes can be divided by a straight line (like AND or OR gates). It cannot learn non-linearly separable problems like XOR.\n",
        "\n",
        "It applies a step function to decide the output.\n",
        "*output = 1 if z ≥ 0 else 0*"
      ],
      "metadata": {
        "id": "qf6ic9NoYq4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inputs and target outputs (AND gate)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Match spreadsheet: initialize weights and bias\n",
        "weights = np.zeros(2)\n",
        "bias = 0\n",
        "learning_rate = 1\n",
        "epochs = 10\n",
        "\n",
        "# Step activation function\n",
        "def step(z):\n",
        "    return 1 if z >= 0 else 0\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_expected = y[i]\n",
        "\n",
        "        z = np.dot(x, weights) + bias\n",
        "        prediction = step(z)\n",
        "        error = y_expected - prediction\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights += learning_rate * error * x\n",
        "        bias += learning_rate * error\n",
        "\n",
        "        print(f\"Input: {x}, Predicted: {prediction}, Error: {error}, Weights: {weights}, Bias: {bias}\")\n"
      ],
      "metadata": {
        "id": "j56ZXIhZeOXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron cannot learn non-linearly separable problems. Below is the proof.\n",
        "# Even if you change epoch to 100 or 1000, error still remains.\n",
        "import numpy as np\n",
        "\n",
        "# XOR input: 2 features\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# XOR output\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.zeros(2)\n",
        "bias = 0\n",
        "learning_rate = 1\n",
        "epochs = 10\n",
        "\n",
        "# Step activation function\n",
        "def step(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        z = np.dot(X[i], weights) + bias\n",
        "        y_pred = step(z)\n",
        "        error = y[i] - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights += learning_rate * error * X[i]\n",
        "        bias += learning_rate * error\n",
        "        total_error += abs(error)\n",
        "\n",
        "        print(f\"Input: {X[i]}, Predicted: {y_pred}, Error: {error}, Weights: {weights}, Bias: {bias}\")\n",
        "\n",
        "    if total_error == 0:\n",
        "        print(\"\\nTraining converged.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "UbjJjqX5v_vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid for AND\n",
        "# Gradient descent and how a basic neural network learns using a single neuron with sigmoid activation.\n",
        "\n",
        "'''\n",
        "This program trains a single-neuron neural network using sigmoid activation to model an AND gate.\n",
        "It uses gradient descent to iteratively update weights and bias by minimizing the mean squared error between predicted\n",
        "and expected outputs. The sigmoid function squashes outputs between 0 and 1, while its derivative is used to compute\n",
        "gradients during backpropagation. The model runs for 10,000 epochs, updating parameters at each step.\n",
        "Every 1000 epochs, it prints the loss to show learning progress. Finally, it prints the network’s predictions for all\n",
        "input combinations, demonstrating how the neuron learns the logic of an AND gate.\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "# Input data (AND gate)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Expected output\n",
        "y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Derivative of sigmoid (for backprop)\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = np.random.randn(2, 1)\n",
        "bias = 0\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Training loop using gradient descent\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    z = np.dot(X, weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "    # Compute loss (MSE)\n",
        "    loss = np.mean((y - y_pred) ** 2)\n",
        "\n",
        "    # Backward pass (gradients)\n",
        "    error = y_pred - y\n",
        "    d_weights = np.dot(X.T, error * sigmoid_derivative(z))\n",
        "    d_bias = np.sum(error * sigmoid_derivative(z))\n",
        "\n",
        "    # Update weights and bias\n",
        "    weights -= learning_rate * d_weights\n",
        "    bias -= learning_rate * d_bias\n",
        "\n",
        "    # Print every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Final output\n",
        "print(\"\\nFinal predictions:\")\n",
        "for i in range(len(X)):\n",
        "    z = np.dot(X[i], weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    print(f\"Input: {X[i]}, Output: {y_pred[0]:.4f}\")\n"
      ],
      "metadata": {
        "id": "SQXiAQL-zdmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding Backpropogation\n",
        "\n",
        "# Inputs: Ears, Whiskers, Fur\n",
        "inputs = [1, 1, 1]\n",
        "\n",
        "# Hidden Layer Weights (2 neurons, each with 3 inputs)\n",
        "hidden_weights = [\n",
        "    [0.6, 0.5, 0.4],  # Neuron 1\n",
        "    [0.3, 0.7, 0.8]   # Neuron 2\n",
        "]\n",
        "hidden_thresholds = [0.9, 1.2]\n",
        "\n",
        "# Output Layer Weights (2 inputs from hidden layer)\n",
        "output_weights = [0.6, 0.9]\n",
        "output_threshold = 1.0\n",
        "\n",
        "# Step 1: Compute hidden layer outputs\n",
        "hidden_outputs = []\n",
        "for i in range(2):\n",
        "    sum_hidden = sum([inputs[j] * hidden_weights[i][j] for j in range(3)])\n",
        "    output = 1 if sum_hidden >= hidden_thresholds[i] else 0\n",
        "    hidden_outputs.append(output)\n",
        "\n",
        "# Step 2: Compute output layer result\n",
        "final_sum = sum([hidden_outputs[i] * output_weights[i] for i in range(2)])\n",
        "final_output = 1 if final_sum >= output_threshold else 0\n",
        "\n",
        "# Step 3: Print result\n",
        "if final_output == 1:\n",
        "    print(\"Mini-brain says: It's a cat!\")\n",
        "else:\n",
        "    print(\"Mini-brain says: Not a cat.\")\n"
      ],
      "metadata": {
        "id": "rIVZWS-WbgOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# House price prediction example:\n",
        "# Inputs and true output\n",
        "# area in sqft and price in lakhs\n",
        "area = 1000\n",
        "true_price = 75\n",
        "\n",
        "# Initial weight\n",
        "weight = 0.01\n",
        "\n",
        "# Predicted price\n",
        "predicted_price = area * weight\n",
        "\n",
        "# Error\n",
        "error = (true_price - predicted_price) ** 2\n",
        "\n",
        "print(\"Predicted Price:\", predicted_price)\n",
        "print(\"Error:\", error)\n",
        "\n",
        "loss = 0.5 * (true_price - predicted_price)**2\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.0001\n",
        "\n",
        "# Gradient computation: dl/dw\n",
        "'''\n",
        "l = 1/2 (y-y1)^2\n",
        "  = 0.5 (y - wx)^2\n",
        "\n",
        "dl/dw =  d  [ 0.5 (y - wx)^2]\n",
        "        ---\n",
        "         dw\n",
        "\n",
        "      = (y - wx) (-x)\n",
        "      =  - (y - y1) * x\n",
        "'''\n",
        "gradient = - (true_price - predicted_price) * area\n",
        "weight = weight - lr * gradient\n",
        "\n",
        "# New prediction after update\n",
        "predicted_price = area * weight\n",
        "print(\"Updated Weight:\", weight)\n",
        "print(\"New Predicted Price:\", predicted_price)\n"
      ],
      "metadata": {
        "id": "q2JC8-6sst2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyTorch**\n",
        "\n",
        "PyTorch is an open-source machine learning library used for deep learning tasks.\n",
        "\n",
        "A tensor is a fundamental concept in PyTorch (and deep learning in general). It's a multi-dimensional array or matrix that can hold data and be manipulated for mathematical operations. Tensors are very similar to NumPy arrays but can run on GPUs (using CUDA) for accelerated computation, making them efficient for deep learning tasks."
      ],
      "metadata": {
        "id": "L-3jEZ8kzPha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "tensor = torch.rand(3, 3)\n",
        "print(tensor)\n"
      ],
      "metadata": {
        "id": "dOsMSP96zPAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1D, 2D and 3D in tensor\n",
        "import torch\n",
        "\n",
        "# 1D Tensor (Vector)\n",
        "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
        "print(\"1D Tensor (shape={}):\\n{}\".format(tensor_1d.shape, tensor_1d))\n",
        "\n",
        "# 2D Tensor (Matrix)\n",
        "tensor_2d = torch.tensor([[1, 2], [3, 4], [5,6]])\n",
        "print(\"\\n2D Tensor (shape={}):\\n{}\".format(tensor_2d.shape, tensor_2d))\n",
        "\n",
        "# 3D Tensor (Stack of Matrices or \"Tensor cube\")\n",
        "tensor_3d = torch.tensor([\n",
        "    [[1,2], [3, 4]],\n",
        "    [[5, 6], [7, 8]]\n",
        "])\n",
        "print(\"\\n3D Tensor (shape={}):\\n{}\".format(tensor_3d.shape, tensor_3d))\n"
      ],
      "metadata": {
        "id": "5F5gSXMz3R1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor Opeerations\n",
        "\n",
        "import torch\n",
        "\n",
        "# Scalar\n",
        "a = torch.tensor(2.0)\n",
        "\n",
        "# Vector\n",
        "v = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# Matrix\n",
        "m = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "\n",
        "# Arithmetic Operatios\n",
        "x = torch.tensor([2.0, 4.0])\n",
        "y = torch.tensor([1.0, 3.0])\n",
        "\n",
        "# Element-wise operations\n",
        "print(x + y)      # Addition\n",
        "print(x * y)      # Multiplication\n",
        "print(x - y)      # Subtraction\n",
        "print(x / y)      # Division\n",
        "\n",
        "\n",
        "# Matrix Multiplication\n",
        "A = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "B = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
        "\n",
        "# Matrix multiplication\n",
        "C = torch.matmul(A, B)\n",
        "print(C)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EprbyoTs3uV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case Study: Predicting Output using y = wx + b\n",
        "# Inputs and parameters\n",
        "x = torch.tensor([2.0])                       # Input\n",
        "w = torch.tensor([3.0], requires_grad=True)  # Weight\n",
        "b = torch.tensor([1.0], requires_grad=True)  # Bias\n",
        "\n",
        "# Forward pass\n",
        "y = w * x + b\n",
        "print(\"Prediction y:\", y)\n"
      ],
      "metadata": {
        "id": "M_yXv9cp4l2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Step 1: Data\n",
        "area = torch.tensor(1.0)         # e.g., 1 unit of area\n",
        "true_price = torch.tensor(3.0)   # e.g., Rs. 3 lakhs\n",
        "\n",
        "# Step 2: Initialize weight with gradient tracking\n",
        "weight = torch.tensor(0.1, requires_grad=True)\n",
        "\n",
        "# Step 3: Learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Step 4: Model function\n",
        "def model(x):\n",
        "    return x * weight\n",
        "\n",
        "# Step 5: Training loop\n",
        "for epoch in range(1000):\n",
        "    # Forward pass\n",
        "    predicted_price = model(area)\n",
        "    loss = (true_price - predicted_price) ** 2\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weight\n",
        "    with torch.no_grad():\n",
        "        weight -= lr * weight.grad\n",
        "\n",
        "    # Zero gradients - reset to zero\n",
        "    weight.grad.zero_()\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 100 == 0 or loss.item() < 1e-6:\n",
        "        print(f\"Epoch {epoch}: Weight = {weight.item():.4f}, Predicted = {predicted_price.item():.4f}, Loss = {loss.item():.6f}\")\n",
        "\n",
        "    if loss.item() < 1e-6:\n",
        "        print(\"Converged.\")\n",
        "        break\n",
        "\n",
        "# Final prediction\n",
        "print(\"\\nFinal Trained Weight:\", weight.item())\n",
        "print(\"Prediction for area=2:\", model(torch.tensor(2.0)).item())\n"
      ],
      "metadata": {
        "id": "bWUGjK425mKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Smart Hiring System**\n",
        "\n",
        "Let us build an application for the smart hiring system.\n",
        "\n"
      ],
      "metadata": {
        "id": "QMIZEcaw8QrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data of smart hiring system\n",
        "import pandas as pd\n",
        "\n",
        "# Candidate data\n",
        "data = {\n",
        "    'GPA': [8.9, 7.5, 6.2, 9.1, 7.0, 5.8, 8.5, 6.5, 5.2, 7.8],\n",
        "    'Intern': [6, 3, 0, 5, 1, 0, 4, 2, 0, 3],\n",
        "    'Projects': [5, 4, 2, 6, 3, 1, 5, 3, 0, 4],\n",
        "    'CommSkill': [9, 7, 5, 8, 6, 4, 9, 6, 3, 8],\n",
        "    'Label': ['Strong Fit', 'Maybe', 'Not a Fit', 'Strong Fit', 'Maybe','Not a Fit', 'Strong Fit', 'Maybe', 'Not a Fit', 'Maybe']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "AGvAxmY38Tcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input for Candidate A\n",
        "x = np.array([8.9, 6, 5, 9])  # Features: GPA, Intern, Projects, CommSkill\n",
        "\n",
        "# Manually defined weights and bias\n",
        "w = np.array([0.3, 0.2, 0.5, 0.1])\n",
        "b = -3\n",
        "\n",
        "# Linear combination\n",
        "z = np.dot(x, w) + b\n",
        "\n",
        "print(\"Weighted sum (Linear Output):\", z)\n"
      ],
      "metadata": {
        "id": "UpiJVLIv9tDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nature of Graphs - Sigmoid, relu and tanh\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate input values from -5 to 5\n",
        "z = np.linspace(-5, 5, 500)\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
        "def relu(z): return np.maximum(0, z)\n",
        "def tanh(z): return np.tanh(z)\n",
        "\n",
        "# Compute outputs\n",
        "sigmoid_vals = sigmoid(z)\n",
        "relu_vals = relu(z)\n",
        "tanh_vals = tanh(z)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(z, sigmoid_vals, label='Sigmoid', color='blue')\n",
        "plt.plot(z, tanh_vals, label='Tanh', color='green')\n",
        "plt.plot(z, relu_vals, label='ReLU', color='red')\n",
        "\n",
        "plt.title('Activation Functions: Sigmoid vs Tanh vs ReLU')\n",
        "plt.xlabel('Input z')\n",
        "plt.ylabel('Activated Output')\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.5)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5aUbVzItMnag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What relu does\n",
        "import numpy as np\n",
        "\n",
        "# Input: Candidate profile\n",
        "x = np.array([8.5, 4, 5, 9])  # Example candidate\n",
        "\n",
        "# Simulate weights for 3 neurons (shape: 3 x 4)\n",
        "W = np.array([\n",
        "    [0.2, 0.5, -0.3, 0.8],   # Neuron 1\n",
        "    [-0.5, 0.1, 0.4, 0.3],   # Neuron 2\n",
        "    [0.6, -0.4, 0.2, 0.1]    # Neuron 3\n",
        "])\n",
        "\n",
        "# Biases for 3 neurons\n",
        "b = np.array([2, -1, 0.5])\n",
        "\n",
        "# ReLU function\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "# Step 1: Weighted sum (dot product + bias)\n",
        "z = np.dot(W, x) + b\n",
        "print(\"Weighted sums (z):\", z)\n",
        "\n",
        "# Step 2: Activation\n",
        "a = relu(z)\n",
        "print(\"Activated outputs (ReLU):\", a)\n"
      ],
      "metadata": {
        "id": "GhjQ10XmR5uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vanishing Gradient Example\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "# Simulate passing a gradient through 10 layers\n",
        "layers = 15\n",
        "initial_gradient = 1\n",
        "x = 8\n",
        "\n",
        "gradient = initial_gradient\n",
        "print(\"Initial gradient:\", gradient)\n",
        "\n",
        "for i in range(1, layers + 1):\n",
        "    gradient *= sigmoid_derivative(x)  # multiply by derivative at each layer\n",
        "    print(f\"After layer {i}, gradient: {gradient}\")\n"
      ],
      "metadata": {
        "id": "1DM6lUJeSiqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting all activation functions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Candidate data\n",
        "data = {\n",
        "    'GPA': [8.9, 7.5, 6.2, 9.1, 7.0, 5.8, 8.5, 6.5, 5.2, 7.8],\n",
        "    'Intern': [6, 3, 0, 5, 1, 0, 4, 2, 0, 3],\n",
        "    'Projects': [5, 4, 2, 6, 3, 1, 5, 3, 0, 4],\n",
        "    'CommSkill': [9, 7, 5, 8, 6, 4, 9, 6, 3, 8],\n",
        "    'Label': ['Strong Fit', 'Maybe', 'Not a Fit', 'Strong Fit', 'Maybe','Not a Fit', 'Strong Fit', 'Maybe', 'Not a Fit', 'Maybe']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Manually defined weights and bias\n",
        "w = np.array([0.3, 0.2, 0.5, 0.1])\n",
        "b = -3\n",
        "\n",
        "# Feature matrix\n",
        "features = df[['GPA', 'Intern', 'Projects', 'CommSkill']].values\n",
        "\n",
        "# Linear output z = w.x + b\n",
        "z_values = np.dot(features, w) + b\n",
        "\n",
        "# Define activation functions\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def tanh(z):\n",
        "  return np.tanh(z)\n",
        "\n",
        "# Apply activations\n",
        "df['z'] = z_values\n",
        "df['Sigmoid'] = sigmoid(z_values)\n",
        "df['ReLU'] = relu(z_values)\n",
        "df['Tanh'] = tanh(z_values)\n",
        "\n",
        "# Display key outputs\n",
        "print(df[['z', 'Sigmoid']])\n",
        "print(df[['z', 'ReLU']])\n",
        "print(df[['z', 'Tanh']])"
      ],
      "metadata": {
        "id": "FRhUNf6YGFyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo of softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "  exp_values = np.exp(z)\n",
        "  return exp_values / np.sum(exp_values)\n",
        "\n",
        "\n",
        "# Example: Scores predicted by a model for one candidate\n",
        "# Let's assume the model predicts 3 raw scores for a candidate:\n",
        "# [Strong Fit, Maybe, Not a Fit]\n",
        "logits = np.array([2.0, 1.0, 0.1])\n",
        "\n",
        "probs = softmax(logits)\n",
        "\n",
        "print(\"Raw scores (logits):\", logits)\n",
        "print(\"Softmax probabilities:\", probs)\n",
        "print(\"Predicted class:\", np.argmax(probs))  # index of highest probability\n"
      ],
      "metadata": {
        "id": "13nb36okV7de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying it to our example\n",
        "import numpy as np\n",
        "\n",
        "# Candidate data: GPA, Intern, Projects, CommSkill\n",
        "X = np.array([\n",
        "    [8.9, 6, 5, 9],\n",
        "    [7.5, 3, 4, 7],\n",
        "    [6.2, 0, 2, 5],\n",
        "    [9.1, 5, 6, 8],\n",
        "    [7.0, 1, 3, 6],\n",
        "    [5.8, 0, 1, 4],\n",
        "    [8.5, 4, 5, 9],\n",
        "    [6.5, 2, 3, 6],\n",
        "    [5.2, 0, 0, 3],\n",
        "    [7.8, 3, 4, 8]\n",
        "])\n",
        "\n",
        "candidates = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]\n",
        "\n",
        "# Let's define weights for each class: [Strong Fit, Maybe, Not a Fit]\n",
        "W = np.array([\n",
        "    [0.4, 0.1, -0.3],  # GPA weights\n",
        "    [0.2, 0.1, -0.2],  # Intern\n",
        "    [0.3, 0.2, -0.1],  # Projects\n",
        "    [0.5, 0.2, -0.4]   # CommSkill\n",
        "])\n",
        "\n",
        "b = np.array([0.5, 0, -0.5])  # Bias for each class\n",
        "\n",
        "def softmax(z):\n",
        "    exp_vals = np.exp(z - np.max(z))\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "\n",
        "labels = [\"Strong Fit\", \"Maybe\", \"Not a Fit\"]\n",
        "\n",
        "for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    z = np.dot(x, W) + b\n",
        "    probs = softmax(z)\n",
        "    prediction = np.argmax(probs)\n",
        "\n",
        "    print(f\"Candidate {candidates[i]}:\")\n",
        "    print(f\"  Raw scores: {z}\")\n",
        "    print(f\"  Probabilities: {probs}\")\n",
        "    print(f\"  Predicted Class: {labels[prediction]}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "W9oxNpBYfWLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple fully connected neural network (MLP) using torch.nn.Module.\n",
        "# MLP - Multi-Layer Perceptron\n",
        "\n",
        "# Import PyTorch and modules for neural networks\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Define Sample Input\n",
        "# -------------------------------\n",
        "\n",
        "# A mini-batch of 2 samples, each with 4 input features\n",
        "# Format: [GPA, Intern months, Projects, Communication Skill] or similar\n",
        "x = torch.tensor([[0.5, -1.2, 0.3, 0.8],\n",
        "                  [0.1,  0.0, -0.5, 0.9]],\n",
        "                 dtype=torch.float32)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Define MLP Architecture\n",
        "# -------------------------------\n",
        "\n",
        "# Subclassing nn.Module to define a custom neural network\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "\n",
        "        # First fully connected layer: input → hidden\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Second fully connected layer: hidden → output\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Hidden layer activation: ReLU (can be replaced with Tanh, Sigmoid, etc.)\n",
        "        self.activation_hidden = nn.ReLU()\n",
        "\n",
        "        # Output layer activation: Softmax to get class probabilities\n",
        "        # dim=1 means softmax is applied across columns (classes) for each sample\n",
        "        self.activation_output = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First layer transformation: linear + activation\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation_hidden(x)\n",
        "\n",
        "        # Second layer transformation: linear + activation\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation_output(x)\n",
        "\n",
        "        # Final output is a probability distribution over 3 classes\n",
        "        return x\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Create Model and Predict\n",
        "# -------------------------------\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size=4, hidden_size=5, output_size=3)\n",
        "\n",
        "# Forward pass: send input through the model\n",
        "output = model(x)\n",
        "\n",
        "# Display output probabilities for each class\n",
        "print(\"Model output:\\n\", output)\n"
      ],
      "metadata": {
        "id": "f-1dx8XwhwIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris Data Set - with forward and back propogation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load and prepare the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Normalize input features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to tensors\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define MLP model\n",
        "class IrisMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisMLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "          nn.Linear(4, 10),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(10, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = IrisMLP()\n",
        "\n",
        "# 3. Define loss function and SGD optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "# 4. Training loop\n",
        "for epoch in range(100):\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 5. Testing\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    _, predictions = torch.max(test_outputs, 1)\n",
        "    accuracy = (predictions == y_test).sum().item() / y_test.size(0)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "mrBGxdws7Mne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with MNIST data set\n",
        "# Wikipedia Link: https://en.wikipedia.org/wiki/MNIST_database\n",
        "# 70,000 grayscale images of handwritten digits\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1. Load and transform MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# 2. Define the MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = MLP()\n",
        "\n",
        "# 3. Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 4. Training loop\n",
        "for epoch in range(5):\n",
        "    running_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()       # Backpropagation\n",
        "        optimizer.step()      # Update weights\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# 5. Test accuracy\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "GqzM2JUEH4lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of NN**\n",
        "\n",
        "Feedforward Neural Networks (FNNs) are the simplest type, where data flows in one direction from input to output, ideal for image and pattern recognition. Recurrent Neural Networks (RNNs) include loops, enabling memory of past inputs, making them suitable for sequential data like text, speech, and time-series predictions."
      ],
      "metadata": {
        "id": "aiyMnkot7Enq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feedforward Neural Network (FNN)\n",
        "How it works:\n",
        "Treats input as a single flat vector: [1, 2, 3]\n",
        "No memory of the order in which numbers came\n",
        "Works okay for simple patterns but doesn’t scale well to sequences or time-dependent data\n",
        "'''\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Training data\n",
        "X_fnn = np.array([[1, 2, 3]])\n",
        "y_fnn = np.array([4])\n",
        "\n",
        "# FNN model\n",
        "fnn = Sequential([\n",
        "    Dense(10, input_shape=(3,), activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "fnn.compile(optimizer='adam', loss='mse')\n",
        "fnn.fit(X_fnn, y_fnn, epochs=200, verbose=0)\n",
        "\n",
        "# Test\n",
        "print(\"FNN prediction for [2, 3, 4]:\", fnn.predict(np.array([[2, 3, 4]])))\n"
      ],
      "metadata": {
        "id": "Mmw1KqwJ7KDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Recurrent Neural Network (RNN)\n",
        "How it works:\n",
        "Treats input as a sequence: [[1], [2], [3]]\n",
        "Processes input one step at a time\n",
        "Maintains memory of past values using hidden states\n",
        "Better suited for sequence tasks like language, stock prices, time series, etc.\n",
        "'''\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Training data reshaped for RNN\n",
        "X_rnn = np.array([[[1], [2], [3]]])  # shape: (batch, time, features)\n",
        "y_rnn = np.array([4])\n",
        "\n",
        "# RNN model\n",
        "rnn = Sequential([\n",
        "    SimpleRNN(10, input_shape=(3, 1)),  # 3 time steps, 1 feature per step\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "rnn.compile(optimizer='adam', loss='mse')\n",
        "rnn.fit(X_rnn, y_rnn, epochs=200, verbose=0)\n",
        "\n",
        "# Test\n",
        "test_input = np.array([[[2], [3], [4]]])  # shape: (1, 3, 1)\n",
        "print(\"RNN prediction for [2, 3, 4]:\", rnn.predict(test_input))\n"
      ],
      "metadata": {
        "id": "dnIteC9r7gWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Keras\n",
        "\n",
        "'''\n",
        "This below code:\n",
        "===================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define weights and biases manually for each layer\n",
        "class SimpleModel(tf.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Layer 1: Dense(64, activation='relu')\n",
        "        self.w1 = tf.Variable(tf.random.normal([10, 64]), name='w1')\n",
        "        self.b1 = tf.Variable(tf.zeros([64]), name='b1')\n",
        "\n",
        "        # Layer 2: Dense(1)\n",
        "        self.w2 = tf.Variable(tf.random.normal([64, 1]), name='w2')\n",
        "        self.b2 = tf.Variable(tf.zeros([1]), name='b2')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # First dense layer with ReLU activation\n",
        "        z1 = tf.matmul(x, self.w1) + self.b1\n",
        "        a1 = tf.nn.relu(z1)\n",
        "\n",
        "        # Output layer (no activation)\n",
        "        z2 = tf.matmul(a1, self.w2) + self.b2\n",
        "        return z2\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleModel()\n",
        "\n",
        "# Example input\n",
        "x = tf.random.normal([5, 10])  # batch of 5 samples, each with 10 features\n",
        "output = model(x)\n",
        "\n",
        "print(\"Model output:\", output)\n",
        "'''\n",
        "\n",
        "\n",
        "# Changes to\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(10,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "hGXoAPls2Efd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering**\n",
        "\n",
        "Clustering is an **unsupervised learning** technique used to group similar data points based on their attributes. It helps identify patterns or structures in data without using predefined labels. The goal is to ensure items within a cluster are more similar to each other than to those in other clusters.\n",
        "\n",
        "Sheet Link: https://docs.google.com/spreadsheets/d/1vjT_HV30xx-DUS30IzVh9maqagWZvJJAMqf6dUPp4S8/edit?gid=0#gid=0"
      ],
      "metadata": {
        "id": "UU0VIzzuCI_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the case study example (Iterations in spreadhseet)\n",
        "import math\n",
        "\n",
        "# Data\n",
        "customers = {\n",
        "    \"C1\": (2, 500),\n",
        "    \"C2\": (10, 800),\n",
        "    \"C3\": (4, 300),\n",
        "    \"C4\": (11, 1200),\n",
        "    \"C5\": (3, 350),\n",
        "    \"C6\": (9, 1000)\n",
        "}\n",
        "\n",
        "# Initialize centroids manually\n",
        "centroid_A = customers[\"C1\"]\n",
        "centroid_B = customers[\"C4\"]\n",
        "\n",
        "def euclidean(p1, p2):\n",
        "    return math.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)\n",
        "\n",
        "def compute_new_centroid(cluster):\n",
        "    freq = sum(c[0] for c in cluster) / len(cluster)\n",
        "    spend = sum(c[1] for c in cluster) / len(cluster)\n",
        "    return (freq, spend)\n",
        "\n",
        "# Run until convergence\n",
        "iteration = 1\n",
        "prev_assignment = {}\n",
        "while True:\n",
        "    print(f\"\\n--- Iteration {iteration} ---\")\n",
        "\n",
        "    cluster_A = []\n",
        "    cluster_B = []\n",
        "    assignment = {}\n",
        "\n",
        "    for cid, (f, s) in customers.items():\n",
        "        dA = euclidean((f, s), centroid_A)\n",
        "        dB = euclidean((f, s), centroid_B)\n",
        "\n",
        "        if dA < dB:\n",
        "            cluster_A.append((f, s))\n",
        "            assignment[cid] = 'A'\n",
        "        else:\n",
        "            cluster_B.append((f, s))\n",
        "            assignment[cid] = 'B'\n",
        "\n",
        "        print(f\"{cid}: Dist to A={dA:.2f}, B={dB:.2f} => Cluster {assignment[cid]}\")\n",
        "\n",
        "    if assignment == prev_assignment:\n",
        "        print(\"\\n Converged.\")\n",
        "        break\n",
        "\n",
        "    prev_assignment = assignment\n",
        "    centroid_A = compute_new_centroid(cluster_A)\n",
        "    centroid_B = compute_new_centroid(cluster_B)\n",
        "\n",
        "    print(f\"\\nNew Centroid A: {centroid_A}\")\n",
        "    print(f\"New Centroid B: {centroid_B}\")\n",
        "\n",
        "    iteration += 1\n",
        "\n",
        "# Final Output\n",
        "print(\"\\n Final Clusters:\")\n",
        "print(\"Cluster A:\", [cid for cid, grp in assignment.items() if grp == 'A'])\n",
        "print(\"Cluster B:\", [cid for cid, grp in assignment.items() if grp == 'B'])\n"
      ],
      "metadata": {
        "id": "h36udF1tCST_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the same using library functions\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Data\n",
        "customer_ids = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\"]\n",
        "data = np.array([\n",
        "    [2, 500],\n",
        "    [10, 800],\n",
        "    [4, 300],\n",
        "    [11, 1200],\n",
        "    [3, 350],\n",
        "    [9, 1000]\n",
        "])\n",
        "\n",
        "# Initial centroids from C1 and C4\n",
        "initial_centroids = np.array([\n",
        "    [2, 500],    # C1\n",
        "    [11, 1200]   # C4\n",
        "])\n",
        "\n",
        "# KMeans model with manually specified initial centroids\n",
        "kmeans = KMeans(n_clusters = 2, init = initial_centroids, n_init = 1, max_iter = 25)\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Results\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Display results\n",
        "for cid, label in zip(customer_ids, labels):\n",
        "    print(f\"{cid} → Cluster {label}\")\n",
        "\n",
        "print(\"\\nCluster Centers:\")\n",
        "print(kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "J68EwWHEFSPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make random initializations\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Data\n",
        "customer_ids = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\"]\n",
        "data = np.array([\n",
        "    [2, 500],\n",
        "    [10, 800],\n",
        "    [4, 300],\n",
        "    [11, 1200],\n",
        "    [3, 350],\n",
        "    [9, 1000]\n",
        "])\n",
        "\n",
        "# KMeans model without manually specified initial centroids\n",
        "kmeans = KMeans(n_clusters = 2, n_init = 10, max_iter = 25)\n",
        "kmeans.fit(data)\n",
        "\n",
        "# Results\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Display results\n",
        "for cid, label in zip(customer_ids, labels):\n",
        "    print(f\"{cid} → Cluster {label}\")\n",
        "\n",
        "print(\"\\nCluster Centers:\")\n",
        "print(kmeans.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "hSBf1j9QJjo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With a graph\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Data\n",
        "customer_ids = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\"]\n",
        "data = np.array([\n",
        "    [2, 500],\n",
        "    [10, 800],\n",
        "    [4, 300],\n",
        "    [11, 1200],\n",
        "    [3, 350],\n",
        "    [9, 1000]\n",
        "])\n",
        "\n",
        "# Initial centroids from C1 and C4\n",
        "initial_centroids = np.array([\n",
        "    [2, 500],    # C1\n",
        "    [11, 1200]   # C4\n",
        "])\n",
        "\n",
        "# Fit KMeans\n",
        "kmeans = KMeans(n_clusters=2, init=initial_centroids, n_init=1, max_iter=100, random_state=42)\n",
        "kmeans.fit(data)\n",
        "labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Plot\n",
        "colors = ['red', 'blue']\n",
        "for i in range(2):\n",
        "    cluster_points = data[labels == i]\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], label=f'Cluster {i}', s=100)\n",
        "\n",
        "# Plot centroids\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=200, label='Centroids')\n",
        "\n",
        "# Add customer IDs as labels\n",
        "for i, txt in enumerate(customer_ids):\n",
        "    plt.annotate(txt, (data[i, 0] + 0.1, data[i, 1] + 10))\n",
        "\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Spend')\n",
        "plt.title('Customer Clusters using K-Means')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AnztBX3YKQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "data = {\n",
        "    'Frequency': [2, 10, 4, 11, 3, 9, 3, 7, 9, 4, 12, 5, 8, 5],\n",
        "    'Spend': [500, 800, 300, 1200, 350, 1000, 750, 800, 1000, 1200, 450, 300, 200, 600]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Try KMeans for different values of k\n",
        "inertia = []\n",
        "K = range(1, 11)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(K, inertia, 'bo-')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hyOPEvXBZMGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Link: https://archive.ics.uci.edu/dataset/352/online+retail\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_excel('Online Retail.xlsx')\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "# Remove the rows with missing customer ids\n",
        "df = df[pd.notnull(df['CustomerID'])]\n",
        "\n",
        "# Remove canceled orders (InvoiceNo starting with 'C')\n",
        "df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
        "\n",
        "# Remove negative quantities\n",
        "df = df[df['Quantity'] > 0]\n",
        "\n",
        "# Step 3: Feature Engineering\n",
        "# Calculate TotalPrice\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Aggregate data by CustomerID\n",
        "customer_df = df.groupby('CustomerID').agg({\n",
        "    'InvoiceNo': 'nunique',  # Frequency\n",
        "    'Quantity': 'sum',\n",
        "    'TotalPrice': 'sum'\n",
        "}).rename(columns={\n",
        "    'InvoiceNo': 'Frequency',\n",
        "    'Quantity': 'TotalQuantity',\n",
        "    'TotalPrice': 'Monetary'\n",
        "}).reset_index()\n",
        "\n",
        "# Step 4: Feature Scaling\n",
        "features = ['Frequency', 'TotalQuantity', 'Monetary']\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(customer_df[features])\n",
        "\n",
        "# Step 5: Determine the optimal number of clusters using the Elbow Method\n",
        "sse = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scaled_features)\n",
        "    sse.append(kmeans.inertia_)\n",
        "\n",
        "print(sse)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Plot the Elbow Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, sse, marker='o')\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Sum of squared distances (Inertia)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Step 6: Evaluate clustering with Silhouette Score\n",
        "silhouette_scores = []\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(scaled_features)\n",
        "    score =  silhouette_score(scaled_features, labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f'For n_clusters = {k}, the Silhouette Score is {score:.4f}')\n",
        "\n",
        "# Plot Silhouette Scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
        "plt.title('Silhouette Scores for Various Clusters')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Step 7: Apply KMeans with the optimal number of clusters (e.g., k=5)\n",
        "optimal_k = 4\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "customer_df['Cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# Step 8: Visualize the clusters\n",
        "# For visualization, we'll use the first two principal components\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "principal_components = pca.fit_transform(scaled_features)\n",
        "customer_df['PC1'] = principal_components[:, 0]\n",
        "customer_df['PC2'] = principal_components[:, 1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(data=customer_df, x='PC1', y='PC2', hue='Cluster', palette='Set2')\n",
        "plt.title('Customer Segments')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0FfdKGU7X-Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchical Clustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample data: 2D points\n",
        "X = np.array([[1, 2], [2, 3], [5, 6], [6, 6], [3, 6], [2, 5], [3, 4], [5,4], [9, 10]])\n",
        "\n",
        "# Create Agglomerative Clustering model\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title(\"Agglomerative Clustering\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NaT4Wf4QRXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2], [2, 3], [5, 6], [6, 6], [3, 6], [2, 5], [3, 4], [5,4], [9, 10]])\n",
        "\n",
        "# Recursive divisive function\n",
        "def divisive_clustering(data, max_clusters=2):\n",
        "    clusters = [data]\n",
        "    labels = np.zeros(len(data), dtype=int)\n",
        "\n",
        "    while len(clusters) < max_clusters:\n",
        "        # Find the largest cluster to split\n",
        "        sizes = [len(c) for c in clusters]\n",
        "        idx = np.argmax(sizes)\n",
        "        to_split = clusters.pop(idx)\n",
        "\n",
        "        # Apply KMeans with 2 clusters\n",
        "        kmeans = KMeans(n_clusters=3, n_init=10)\n",
        "        kmeans.fit(to_split)\n",
        "        split_labels = kmeans.labels_\n",
        "\n",
        "        # Split the cluster\n",
        "        cluster1 = to_split[split_labels == 0]\n",
        "        cluster2 = to_split[split_labels == 1]\n",
        "        clusters.extend([cluster1, cluster2])\n",
        "\n",
        "    # Assign final labels\n",
        "    label_id = 0\n",
        "    for cluster in clusters:\n",
        "        for point in cluster:\n",
        "            index = np.where((X == point).all(axis=1))[0][0]\n",
        "            labels[index] = label_id\n",
        "        label_id += 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "# Apply divisive clustering\n",
        "labels = divisive_clustering(X, max_clusters=2)\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.title(\"Divisive Clustering (via KMeans)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X-2N6xflRrPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading a Dendrogram\n",
        "from sklearn.datasets import make_blobs\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "X, _ = make_blobs(n_samples=30, centers=3, random_state=42)\n",
        "\n",
        "# Step 2: Compute the linkage matrix\n",
        "Z = linkage(X, method='complete')  # You can try 'ward, 'single', 'complete', 'average'\n",
        "\n",
        "# Step 3: Plot dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.xlabel(\"Data Point Index\")\n",
        "plt.ylabel(\"Cluster Distance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Pfe2BC4rII9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Step 1: Generate synthetic dataset\n",
        "X, _ = make_blobs(n_samples=30, centers=3, random_state=42)\n",
        "\n",
        "# Step 2: List of linkage methods\n",
        "methods = ['ward', 'single', 'complete', 'average']\n",
        "\n",
        "# Step 3: Set up a 2x2 plot grid\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Step 4: Loop through each method and plot the dendrogram\n",
        "for i, method in enumerate(methods):\n",
        "    Z = linkage(X, method=method)\n",
        "    dendrogram(Z, ax=axs[i])\n",
        "    axs[i].set_title(f'Dendrogram - {method.capitalize()} Linkage')\n",
        "    axs[i].set_xlabel('Sample Index')\n",
        "    axs[i].set_ylabel('Distance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vgv6E36l30um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying DBSCAN on 2-moons data set\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Understand the dataset\n",
        "X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.title(\"Two Moons Dataset\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# What would k-means do to this?\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='rainbow', s=40)\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DBSCAN with chosen parameters\n",
        "db = DBSCAN(eps=0.2, min_samples = 5)\n",
        "db.fit(X)\n",
        "\n",
        "# Get labels (-1 means noise)\n",
        "labels = db.labels_\n",
        "\n",
        "# Plotting\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=40)\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Filter out noise points\n",
        "core_samples_mask = labels != -1\n",
        "if np.sum(core_samples_mask) > 1:\n",
        "    score = silhouette_score(X[core_samples_mask], labels[core_samples_mask])\n",
        "    print(f\"Silhouette Score: {score:.2f}\")\n",
        "else:\n",
        "    print(\"Not enough core samples for silhouette score.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iRt7UWv86RvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measuring Accuracy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Step 1: Generate two-moons dataset\n",
        "X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "\n",
        "# Step 2: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Step 3: Plot clustering result\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=40)\n",
        "plt.title(\"DBSCAN Clustering Result\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Filter out noise for evaluation (-1 are noise points)\n",
        "mask = labels != -1\n",
        "X_core = X[mask]\n",
        "labels_core = labels[mask]\n",
        "\n",
        "# Step 5: Evaluation\n",
        "if len(set(labels_core)) > 1:\n",
        "    sil_score = silhouette_score(X_core, labels_core)\n",
        "    db_score = davies_bouldin_score(X_core, labels_core)\n",
        "    print(f\"Silhouette Score: {sil_score:.2f}\")\n",
        "    print(f\"Davies-Bouldin Index: {db_score:.2f}\")\n",
        "else:\n",
        "    print(\"Not enough clusters (or too many noise points) for evaluation.\")\n"
      ],
      "metadata": {
        "id": "s-sdicHzO8i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Step 1: Generate synthetic clustered data with noise\n",
        "X, _ = make_blobs(n_samples=400, centers=3, cluster_std=0.6, random_state=42)\n",
        "noise = np.random.uniform(low=-10, high=10, size=(40, 2))  # Add some outliers\n",
        "X = np.vstack([X, noise])  # Combine clusters and outliers\n",
        "\n",
        "# Step 2: Try DBSCAN with different eps values\n",
        "# In DBSCAN, eps (epsilon) defines the radius of a point's neighborhood. It controls how close points must be to be considered part of the same cluster.\n",
        "# A point is labeled a core point if it has enough neighbors (min_samples) within this eps radius.\n",
        "# Small eps values can lead to many outliers or fragmented clusters, while large values may merge distinct clusters.\n",
        "# It directly affects the number and shape of clusters detected. Choosing the right eps is crucial and often done through visual inspection or a k-distance plot.\n",
        "#DBSCAN is powerful for detecting clusters of arbitrary shape and noise.\n",
        "eps_values = [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "# Step 3: Plot the results\n",
        "plt.figure(figsize=(16, 8))\n",
        "for i, eps in enumerate(eps_values, 1):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels = dbscan.fit_predict(X)\n",
        "\n",
        "    plt.subplot(2, 2, i)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired', s=30)\n",
        "    plt.title(f\"DBSCAN with eps={eps} | Clusters: {len(set(labels)) - (1 if -1 in labels else 0)} | Outliers: {np.sum(labels==-1)}\")\n",
        "    plt.xlabel(\"X1\")\n",
        "    plt.ylabel(\"X2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q2GMlQ5lRBbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Processing\n",
        "\n",
        "Sheet Link: https://docs.google.com/spreadsheets/d/1EAzPQG5aDgp0aHrYeQNOml5SeIZXXCFFHIf3NHaehsQ/edit?gid=818603946#gid=818603946"
      ],
      "metadata": {
        "id": "aQPudoeM08u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and convert to RGB\n",
        "image = cv2.imread('iit-ropar.jpg')\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Split RGB channels\n",
        "r = image_rgb[:, :, 0]\n",
        "g = image_rgb[:, :, 1]\n",
        "b = image_rgb[:, :, 2]\n",
        "\n",
        "# Sum of pixel values in each channel\n",
        "total_r = np.sum(r)\n",
        "total_g = np.sum(g)\n",
        "total_b = np.sum(b)\n",
        "\n",
        "# Plotting\n",
        "colors = ['Red', 'Green', 'Blue']\n",
        "values = [total_r, total_g, total_b]\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "plt.title(\"Original RGB Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(colors, values, color=['red', 'green', 'blue'])\n",
        "plt.title('Total Color Intensity in Image')\n",
        "plt.ylabel('Sum of Pixel Values')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0HHCGmmcZwQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Motivation Case Study\n",
        "# Why do you see what do you see?\n",
        "# Why is this blurry?\n",
        "# But let us stitch the story better\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Load an image from CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "image, label = dataset[0]  # Get the first image and label\n",
        "\n",
        "# Simulate a bounding box — format: (x_min, y_min, width, height)\n",
        "bbox = [10, 10, 20, 15]  # You can change this to any values\n",
        "\n",
        "# Plot the image with a bounding box\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image.permute(1, 2, 0), interpolation='nearest')  # Convert from (C, H, W) to (H, W, C) for plotting\n",
        "\n",
        "# Create a Rectangle patch\n",
        "rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)\n",
        "\n",
        "# Optional: add label text\n",
        "ax.text(bbox[0], bbox[1] - 2, 'Object', color='red', fontsize=10, backgroundcolor='white')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PDwGz57J0h7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the image using OpenCV (this loads in BGR format)\n",
        "image_bgr = cv2.imread('iit-ropar.jpg')\n",
        "\n",
        "# Step 2: Convert the image to RGB format\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB )\n",
        "\n",
        "# Step 3: Display both images side by side using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Display BGR image (will look odd)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_bgr)  # matplotlib expects RGB, so this will show wrong colors\n",
        "plt.title(\"Loaded Image (BGR)\")\n",
        "plt.axis('off')\n",
        "\n",
        "# Display RGB image (correct colors)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image_rgb)\n",
        "plt.title(\"Converted Image (RGB)\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k7XnzKk3h0ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load an image\n",
        "image = cv2.imread('iit-ropar.jpg')\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display shape and dtype\n",
        "print(\"Shape:\", image_rgb.shape)  # e.g., (height, width, channels)\n",
        "print(\"Data Type:\", image_rgb.dtype)\n",
        "\n",
        "# Plot RGB channels separately\n",
        "r, g, b = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "for i, channel in enumerate([r, g, b]):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.imshow(channel, cmap='gray')\n",
        "    plt.title(['Red', 'Green', 'Blue'][i])\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O83sqaQzdA2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load an image\n",
        "image = cv2.imread('iit-ropar.jpg')\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Split channels\n",
        "r, g, b = image_rgb[:,:,0], image_rgb[:,:,1], image_rgb[:,:,2]\n",
        "\n",
        "# Create zero channel\n",
        "zeros = np.zeros_like(r)\n",
        "\n",
        "# Stack each channel with two zeroed channels to isolate color\n",
        "red_image   = np.stack([r, zeros, zeros], axis=2)\n",
        "green_image = np.stack([zeros, g, zeros], axis=2)\n",
        "blue_image  = np.stack([zeros, zeros, b], axis=2)\n",
        "\n",
        "# Plot all three\n",
        "plt.figure(figsize=(10, 3))\n",
        "for i, channel_img in enumerate([red_image, green_image, blue_image]):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.imshow(channel_img)\n",
        "    plt.title(['Red Channel', 'Green Channel', 'Blue Channel'][i])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RLQK2DQediy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Processing\n",
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "img = Image.open(\"iit-ropar.jpg\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "processed_img = transform(img)\n",
        "print(\"Transformed shape:\", processed_img.shape)  # (C, H, W)\n"
      ],
      "metadata": {
        "id": "9u5hWzLueNYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with a sample convolution\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Build the example image\n",
        "input = torch.tensor([[[\n",
        "                        [1., 2., 1., 0., 0.],\n",
        "                        [0., 1., 2., 1., 0.],\n",
        "                        [1., 2., 1., 0., 0.],\n",
        "                        [0., 1., 2., 1., 0.],\n",
        "                        [0., 0., 1., 2., 1.]\n",
        "                        ]]])  # shape = (1, 1, 5, 5)\n",
        "\n",
        "# Convolution filter (e.g., edge detection)\n",
        "conv = nn.Conv2d(in_channels = 1,out_channels = 1,kernel_size = 3, stride =1, padding = 1 )\n",
        "conv.weight.data = torch.tensor([[[\n",
        "                                [1., 0., -1.],\n",
        "                                [1., 0., -1.],\n",
        "                                [1., 0., -1.]\n",
        "                                ]]])\n",
        "\n",
        "output = conv(input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "_G7iyO8MbWof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a convolutional layer manually\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)\n",
        "\n",
        "# Assign the custom vertical edge detection kernel\n",
        "conv.weight.data = torch.tensor([[[[1., 0., -1.],\n",
        "                                   [1., 0., -1.],\n",
        "                                   [1., 0., -1.]]]])\n",
        "\n",
        "# Create a dummy 5x5 image\n",
        "image = torch.tensor([[[\n",
        "    [10., 10., 20., 30., 30.],\n",
        "    [10., 10., 20., 30., 30.],\n",
        "    [10., 10., 20., 30., 30.],\n",
        "    [10., 10., 20., 30., 30.],\n",
        "    [10., 10., 20., 30., 30.]\n",
        "]]])  # shape = (1, 1, 5, 5)\n",
        "\n",
        "# Apply the filter\n",
        "output = conv(image)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "f_uPMOXhovps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Single layer example\n",
        "layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "input_image = torch.randn(1, 1, 28, 28)  # batch_size x channels x height x width\n",
        "output = layer(input_image)\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "id": "Q68F1aTnxv8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with CIFAR-10 dataset\n",
        "# Link: https://www.kaggle.com/c/cifar-10/m\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load and Normalize CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 3-channel mean/std\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Define a Simple CNN\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# 3. Initialize Model, Loss, Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 4. Training Loop\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_acc = correct / total\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.4f}\")\n",
        "\n",
        "# 5. Plot Loss and Accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v-HZRi4ltuvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 2 leverages transfer learning using a pretrained ResNet18 model.\n",
        "# By freezing early layers and fine-tuning only the final layer, it efficiently adapts to\n",
        "# CIFAR-10 image classification. This method reduces training time, improves generalization, and\n",
        "# demonstrates how pretrained models accelerate deep learning tasks on smaller datasets with limited resources.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import time\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 1. Data Transformation and Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize CIFAR images to 224x224 for ResNet18\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2)\n",
        "\n",
        "# 2. Load Pretrained ResNet18 and Modify Final Layer\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer (fc)\n",
        "num_ftrs = resnet18.fc.in_features\n",
        "resnet18.fc = nn.Linear(num_ftrs, 10)  # CIFAR-10 has 10 classes\n",
        "\n",
        "resnet18 = resnet18.to(device)\n",
        "\n",
        "# 3. Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet18.fc.parameters(), lr=0.001)  # Only train the final layer\n",
        "\n",
        "# 4. Training Loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    resnet18.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet18(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# 5. Testing Accuracy\n",
        "resnet18.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = resnet18(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Sz7CPp_B_9G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Anchor box (predicted) and Ground truth\n",
        "pred_box = [50, 50, 100, 100]  # x, y, width, height\n",
        "gt_box = [60, 60, 80, 80]\n",
        "\n",
        "def draw_box(ax, box, label, color):\n",
        "    rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=2, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    ax.text(box[0], box[1] - 10, label, color=color, fontsize=12)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "draw_box(ax, pred_box, \"Pred\", 'blue')\n",
        "draw_box(ax, gt_box, \"GT\", 'green')\n",
        "plt.xlim(0, 200)\n",
        "plt.ylim(0, 200)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"Anchor Box (Blue) vs Ground Truth (Green)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zvDWs9jrJEW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Case Study Image: Just a blank placeholder for drawing boxes\n",
        "image = np.ones((100, 100))\n",
        "\n",
        "# Ground Truth box: [x1, y1, x2, y2]\n",
        "gt_box = [30, 30, 70, 70]\n",
        "\n",
        "# Anchor Boxes (proposed regions)\n",
        "anchors = [\n",
        "    [25, 25, 75, 75],   # Good match\n",
        "    [40, 40, 90, 90],   # Partial overlap\n",
        "    [10, 10, 50, 50]    # Poor match\n",
        "]\n",
        "\n",
        "# Predicted scores for each anchor (confidence)\n",
        "scores = [0.9, 0.6, 0.3]\n",
        "\n",
        "# IoU function\n",
        "def iou(box1, box2):\n",
        "    xi1, yi1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n",
        "    xi2, yi2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n",
        "    inter = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union = area1 + area2 - inter\n",
        "    return inter / union if union != 0 else 0\n",
        "\n",
        "# Visualize GT and Anchors\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(image, cmap='gray')\n",
        "ax.add_patch(plt.Rectangle((gt_box[0], gt_box[1]), gt_box[2] - gt_box[0], gt_box[3] - gt_box[1],\n",
        "             edgecolor='green', lw=2, fill=False, label='GT'))\n",
        "\n",
        "for i, box in enumerate(anchors):\n",
        "    ax.add_patch(plt.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1],\n",
        "                 edgecolor='red', linestyle='--', lw=1.5, fill=False))\n",
        "    ax.text(box[0], box[1] - 3, f\"A{i+1} ({scores[i]:.1f})\", color='red', fontsize=7)\n",
        "\n",
        "plt.title(\"GT and Anchor Boxes\")\n",
        "plt.axis('off')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 1: IoU\n",
        "print(\"IoUs with Ground Truth:\")\n",
        "ious = [iou(box, gt_box) for box in anchors]\n",
        "for i, val in enumerate(ious):\n",
        "    print(f\"Anchor {i+1}: IoU = {val:.2f}\")\n",
        "\n",
        "# Step 2: Sort predictions by score\n",
        "sorted_preds = sorted(zip(anchors, scores, ious), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Step 3: Calculate TP, FP\n",
        "tp, fp = [], []\n",
        "matched = False\n",
        "for box, score, iou_val in sorted_preds:\n",
        "    if iou_val >= 0.5 and not matched:\n",
        "        tp.append(1)\n",
        "        fp.append(0)\n",
        "        matched = True  # Only one GT\n",
        "    else:\n",
        "        tp.append(0)\n",
        "        fp.append(1)\n",
        "\n",
        "# Step 4: Precision, Recall\n",
        "tp_cum = np.cumsum(tp)\n",
        "fp_cum = np.cumsum(fp)\n",
        "precision = tp_cum / (tp_cum + fp_cum)\n",
        "recall = tp_cum / 1  # Only 1 GT\n",
        "\n",
        "print(\"\\nPrecision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Step 5: Plot PR curve\n",
        "plt.plot(recall, precision, marker='o')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 6: mAP = area under PR curve\n",
        "mAP = np.trapezoid(precision, recall)\n",
        "print(f\"\\nmAP: {mAP:.2f}\")\n"
      ],
      "metadata": {
        "id": "o4zUE-anCBmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Revision Sessions**\n",
        "\n",
        "The following codes were used during revision sessions."
      ],
      "metadata": {
        "id": "1LbpvHfiOpk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sheet Link\n",
        "# https://docs.google.com/spreadsheets/d/1JdWoWGBs-Ztj6Ig2KqRz-Afh8xf-QEJmV5e0SMXN1bQ/edit?gid=0#gid=0\n",
        "# Linear and Polynomial Regression Template\n",
        "\n",
        "# 1. Import Libraries\n",
        "numpy, sklearn, matplotlib\n",
        "\n",
        "# 2. Load Data\n",
        "X\n",
        "y\n",
        "\n",
        "# 3. Fit the Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X,y)\n",
        "y_pred_lin = lin_reg.predict(X)\n",
        "\n",
        "# Polynomial features\n",
        "poly_features = PolynomialFeatures(degree=)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "poly_reg = LinearRegression()\n",
        "poly_reg.fit(X_poly, y)\n",
        "y_pred_poly = poly_reg.predict(X_poly)\n",
        "\n",
        "# 4. Metrics\n",
        "r2\n",
        "mse\n",
        "\n",
        "# 5. Plot the graph\n",
        "plt."
      ],
      "metadata": {
        "id": "P-AMXAJqdvmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear and Polynomial Regression\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate simple data\n",
        "# Traffic jam severity (independent variable)\n",
        "X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "\n",
        "# Footfall per hour (dependent variable)\n",
        "y = np.array([500, 345, 450, 420, 400, 421, 350, 330, 310, 456, 270])\n",
        "\n",
        "# Linear Regression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "y_pred_lin = lin_reg.predict(X)\n",
        "\n",
        "# Polynomial Regression\n",
        "poly_features = PolynomialFeatures(degree=8)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "poly_reg = LinearRegression()\n",
        "poly_reg.fit(X_poly, y)\n",
        "y_pred_poly = poly_reg.predict(X_poly)\n",
        "\n",
        "# Metrics\n",
        "print(\"Linear Regression:\")\n",
        "print(\"R² Score:\", r2_score(y, y_pred_lin))\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_lin))\n",
        "\n",
        "print(\"\\nPolynomial Regression:\")\n",
        "print(\"R² Score:\", r2_score(y, y_pred_poly))\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred_poly))\n",
        "\n",
        "# Sort for smooth plotting\n",
        "X_sorted = np.sort(X, axis=0)\n",
        "y_pred_lin_sorted = lin_reg.predict(X_sorted)\n",
        "X_poly_sorted = poly_features.transform(X_sorted)\n",
        "y_pred_poly_sorted = poly_reg.predict(X_poly_sorted)\n",
        "\n",
        "# Plot 1: Linear Regression\n",
        "plt.figure()\n",
        "plt.scatter(X, y, color='blue', label='Original data')\n",
        "plt.plot(X_sorted, y_pred_lin_sorted, color='green', label='Linear fit')\n",
        "plt.legend()\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot 2: Polynomial Regression\n",
        "plt.figure()\n",
        "plt.scatter(X, y, color='blue', label='Original data')\n",
        "plt.plot(X_sorted, y_pred_poly_sorted, color='red', label='Polynomial fit')\n",
        "plt.legend()\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Polynomial Regression\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8tSRNPrmdq2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Home work Exercise\n",
        "# Metro Interstate Traffic Volume\n",
        "# Download the data set here\n",
        "# https://archive.ics.uci.edu/dataset/492/metro+interstate+traffic+volume\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Metro_Interstate_Traffic_Volume.csv')\n",
        "\n",
        "# Feature engineering: Convert date_time to hour and day of week\n",
        "# df['date_time'] = pd.to_datetime(df['date_time'])\n",
        "# df['hour'] = df['date_time'].dt.hour\n",
        "# df['day_of_week'] = df['date_time'].dt.dayofweek\n",
        "\n",
        "# Select relevant features\n",
        "features = ['clouds_all'] # also experiment by adding others and by adding more than one\n",
        "X = df[features]\n",
        "y = df['traffic_volume']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3, label='Predicted vs Actual', color='blue')\n",
        "\n",
        "\n",
        "plt.xlabel('Actual Traffic Volume')\n",
        "plt.ylabel('Predicted Traffic Volume')\n",
        "plt.title('Actual vs Predicted Traffic Volume')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3N4cyFv4yzbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PageRank Example\n",
        "# Link for sheet\n",
        "# https://docs.google.com/spreadsheets/d/1AEnaRRDHRzVczEuL8RGekGRy94q6gaV7yihjZiWzBGk/edit?gid=827786899#gid=827786899\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add directed edges between nodes\n",
        "edges = [\n",
        "    ('A', 'B'),\n",
        "    ('B', 'C'),\n",
        "    ('C', 'A'),\n",
        "    ('D', 'C'),\n",
        "    ('E', 'C'),\n",
        "    ('E', 'D')\n",
        "]\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Compute PageRank\n",
        "pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
        "\n",
        "# Print results\n",
        "print(\"PageRank scores:\")\n",
        "for node, score in pagerank_scores.items():\n",
        "    print(f\"{node}: {score:.4f}\")\n"
      ],
      "metadata": {
        "id": "enJHmHdBcG55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PageRank\n",
        "# Stanford Web Graph\n",
        "# Link: https://snap.stanford.edu/data/web-Stanford.html\n",
        "\n",
        "import gzip\n",
        "import networkx as nx\n",
        "\n",
        "# Initialize directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Path to the downloaded .gz file\n",
        "file_path = \"web-Stanford.txt.gz\"\n",
        "\n",
        "# Load the graph from file\n",
        "with gzip.open(file_path, 'rt') as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"#\"):\n",
        "            continue  # Skip comments\n",
        "        src, dst = map(int, line.strip().split())\n",
        "        G.add_edge(src, dst)\n",
        "\n",
        "print(f\"Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "\n",
        "# Compute PageRank\n",
        "pagerank_scores = nx.pagerank(G, alpha=0.85)\n",
        "\n",
        "# Display top 10 pages by PageRank\n",
        "top_10 = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top 10 nodes by PageRank:\")\n",
        "for node, score in top_10:\n",
        "    print(f\"Node {node}: {score:.6f}\")"
      ],
      "metadata": {
        "id": "ovA1SfBxVPiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron\n",
        "# Working With Mushroom data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load and preprocess Mushroom dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
        "columns = [\n",
        "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
        "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
        "    'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
        "    'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
        "    'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
        "    'ring-type', 'spore-print-color', 'population', 'habitat'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(url, header=None, names=columns)\n",
        "df = df.replace('?', np.nan).dropna()\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop('class', axis=1)\n",
        "y = (df['class'] == 'p').astype(int)  # 1 = poisonous, 0 = edible\n",
        "\n",
        "# One-hot encode all categorical features\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Binary cross-entropy loss\n",
        "def compute_loss(y_true, y_pred):\n",
        "    epsilon = 1e-10  # to avoid log(0)\n",
        "    return -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "# Gradient Descent Training\n",
        "def train_gradient_descent(X, y, lr=0.1, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    weights = np.zeros(n)\n",
        "    bias = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        linear_output = np.dot(X, weights) + bias\n",
        "        predictions = sigmoid(linear_output)\n",
        "\n",
        "        # Compute gradients\n",
        "        dw = np.dot(X.T, (predictions - y)) / m\n",
        "        db = np.sum(predictions - y) / m\n",
        "\n",
        "        # Update weights\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "        # Optionally print loss every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            loss = compute_loss(y, predictions)\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Train the model\n",
        "weights, bias = train_gradient_descent(X_train, y_train, lr=0.5, epochs=1000)\n",
        "\n",
        "# Prediction\n",
        "def predict(X, weights, bias, threshold=0.5):\n",
        "    return (sigmoid(np.dot(X, weights) + bias) >= threshold).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = predict(X_test, weights, bias)\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Edible', 'Poisonous']))\n"
      ],
      "metadata": {
        "id": "tnNqSwx8v0D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crearting a pipeline for Mushroom dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load mushroom dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data'\n",
        "columns = [\n",
        "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
        "    'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape',\n",
        "    'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\n",
        "    'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color',\n",
        "    'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat'\n",
        "]\n",
        "df = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "# Clean data\n",
        "df = df.replace('?', np.nan).dropna()\n",
        "\n",
        "# Separate features and label\n",
        "X = df.drop('class', axis=1)\n",
        "y = (df['class'] == 'p').astype(int)  # Poisonous = 1, Edible = 0\n",
        "\n",
        "# One-hot encoding and scaling using a pipeline\n",
        "categorical_features = X.columns.tolist()\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "])\n",
        "\n",
        "# Create full pipeline with logistic regression\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessing\", preprocessor),\n",
        "    (\"scaling\", StandardScaler(with_mean=False)),  # use with_mean=False for sparse output\n",
        "    (\"classifier\", LogisticRegression(solver='lbfgs', max_iter=1000))\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Edible\", \"Poisonous\"]))\n"
      ],
      "metadata": {
        "id": "Xws19dl0fO5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction**\n",
        "\n",
        "Dimensionality reduction involves reducing the number of input features in a dataset to simplify analysis and improve performance. It includes feature selection (choosing key variables) and feature extraction (creating new, compact features). This process helps avoid the curse of dimensionality, reduces overfitting, improves model training time, enhances visualization, and filters out noise. A popular technique is Principal Component Analysis (PCA), which linearly transforms data into uncorrelated components (principal components) ordered by variance. Key ideas in PCA include the covariance matrix, eigenvectors (direction), eigenvalues (variance magnitude), and the explained variance ratio, which shows each component’s contribution to the total variance.\n",
        "\n",
        "**Sheet Link:**\n",
        "https://docs.google.com/spreadsheets/d/1_myOs1K_vTUC1EbcjUy9K6fq1iRfdKDTOWZSANLg0dU/edit?gid=0#gid=0"
      ],
      "metadata": {
        "id": "VPnb8R4w2vat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 01: Understanding the need\n",
        "# =================================\n",
        "\n",
        "# Learning with a synthetic data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create synthetic 2D dataset\n",
        "np.random.seed(0)\n",
        "x = np.random.normal(0, 1, 100)\n",
        "y = 2 * x + np.random.normal(0, 1, 100)\n",
        "X = np.vstack((x, y)).T\n",
        "\n",
        "# Plot original data\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.5)\n",
        "plt.title(\"Original 2D Data\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.axis('equal')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XLcPIpzK20Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does this output mean? Explain\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.5)\n",
        "plt.title(\"Data after PCA\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_69fifLB3JJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2 - Asking Questions\n",
        "# ========================================================\n",
        "# Dataset Details - Data set from scikit-learn\n",
        "# Name: Digits Dataset\n",
        "# Content: 8×8 grayscale images of handwritten digits (0–9)\n",
        "\n",
        "# What do you infer from below code?\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ----------------- Without PCA -----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Accuracy without PCA:\", accuracy_score(y_test, clf.predict(X_test)))\n",
        "\n",
        "# ----------------- With PCA -----------------\n",
        "pca = PCA(n_components=20)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, random_state=42)\n",
        "clf.fit(X_train_pca, y_train_pca)\n",
        "print(\"Accuracy with PCA:\", accuracy_score(y_test_pca, clf.predict(X_test_pca)))\n",
        "\n"
      ],
      "metadata": {
        "id": "nsyr-UYW3RAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# California Housing Dataset\n",
        "# Link: https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ----------------- Without PCA -----------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)\n",
        "\n",
        "start_time_no_pca = time.time()\n",
        "model_no_pca = LinearRegression()\n",
        "model_no_pca.fit(X_train, y_train)\n",
        "end_time_no_pca = time.time()\n",
        "\n",
        "y_pred_no_pca = model_no_pca.predict(X_test)\n",
        "mse_no_pca = mean_squared_error(y_test, y_pred_no_pca)\n",
        "training_time_no_pca = end_time_no_pca - start_time_no_pca\n",
        "\n",
        "print(\"Without PCA:\")\n",
        "print(\"MSE:\", mse_no_pca)\n",
        "print(\"Training Time:\", training_time_no_pca)\n",
        "\n",
        "# ----------------- With PCA -----------------\n",
        "pca = PCA(n_components=5)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, random_state=42)\n",
        "\n",
        "start_time_pca = time.time()\n",
        "model_pca = LinearRegression()\n",
        "model_pca.fit(X_train_pca, y_train_pca)\n",
        "end_time_pca = time.time()\n",
        "\n",
        "y_pred_pca = model_pca.predict(X_test_pca)\n",
        "mse_pca = mean_squared_error(y_test_pca, y_pred_pca)\n",
        "training_time_pca = end_time_pca - start_time_pca\n",
        "\n",
        "print(\"\\nWith PCA:\")\n",
        "print(\"MSE:\", mse_pca)\n",
        "print(\"Training Time:\", training_time_pca)\n"
      ],
      "metadata": {
        "id": "Dxi633Uw1Y-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Breast Cancer Wisconsin dataset\n",
        "# Link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---- Without PCA ----\n",
        "start_time = time.time()\n",
        "lr = LogisticRegression(max_iter = 1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "preds = lr.predict(X_test_scaled)\n",
        "end_time = time.time()\n",
        "accuracy_no_pca = accuracy_score(y_test, preds)\n",
        "time_no_pca = end_time - start_time\n",
        "\n",
        "# ---- With PCA ----\n",
        "pca = PCA(n_components=10)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "start_time = time.time()\n",
        "lr_pca = LogisticRegression(max_iter = 1000)\n",
        "lr_pca.fit(X_train_pca, y_train)\n",
        "preds_pca = lr_pca.predict(X_test_pca)\n",
        "end_time = time.time()\n",
        "accuracy_pca = accuracy_score(y_test, preds_pca)\n",
        "time_pca = end_time - start_time\n",
        "\n",
        "# ---- Results ----\n",
        "print(f\"Without PCA - Accuracy: {accuracy_no_pca:.4f}, Time: {time_no_pca:.4f} seconds\")\n",
        "print(f\"With PCA    - Accuracy: {accuracy_pca:.4f}, Time: {time_pca:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "EJw2M0JQ7hDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IRIS data set visualization\n",
        "# Can we make it better?\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Create DataFrame including labels\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['Species'] = [target_names[i] for i in y]\n",
        "\n",
        "# Plot pairwise scatterplots\n",
        "sns.pairplot(df, hue='Species', palette='Set2', markers=[\"o\", \"s\", \"D\"])\n",
        "plt.suptitle(\"Pairwise Scatterplots of Iris Dataset Features\", y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eJ05zDXo_NB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How can we do better?\n",
        "# t-SNE?\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity = 30, max_iter = 1000)\n",
        "X_embedded = tsne.fit_transform(X_scaled)\n",
        "\n",
        "# Create DataFrame for plotting\n",
        "df_tsne = pd.DataFrame()\n",
        "df_tsne[\"Component 1\"] = X_embedded[:, 0]\n",
        "df_tsne[\"Component 2\"] = X_embedded[:, 1]\n",
        "df_tsne[\"Label\"] = [target_names[i] for i in y]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=df_tsne, x=\"Component 1\", y=\"Component 2\", hue=\"Label\", palette=\"Set2\", s=100)\n",
        "plt.title(\"t-SNE on Iris Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mBkLi3B5-u06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The 20 newsgroups text dataset\n",
        "# Link: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Load text data (subset for speed)\n",
        "categories = ['rec.sport.baseball', 'sci.med', 'comp.graphics', 'talk.politics.misc']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# 2. Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=2000, stop_words='english')\n",
        "X = vectorizer.fit_transform(data.data).toarray()\n",
        "y = data.target\n",
        "\n",
        "# 3. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# 5. DataLoader\n",
        "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_ds = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=32)\n",
        "\n",
        "# 6. Feedforward NN model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2000,512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512,128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128,len(categories))\n",
        ")\n",
        "\n",
        "# 7. Training setup\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 8. Training loop\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss, correct = 0, 0\n",
        "    for xb, yb in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = loss_fn(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += (out.argmax(1) == yb).sum().item()\n",
        "    acc = correct / len(train_ds)\n",
        "    print(f\"Epoch {epoch+1}: Loss={total_loss:.2f}, Accuracy={acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "4Bfk1T2SfSqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IRIS dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and preprocess data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10,3)\n",
        ")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(300):\n",
        "    # Forward\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    predicted = torch.argmax(test_outputs, dim=1)\n",
        "    accuracy = (predicted == y_test).float().mean()\n",
        "    print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "PH7bD6dwE1tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: Build an efficient Neural Network for Wine Data Set\n",
        "# Link: https://archive.ics.uci.edu/dataset/109/wine\n",
        "# Use 50 Epochs at max\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train, y_train)\n",
        "test_ds = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=16)\n",
        "\n",
        "# Define model\n",
        "class WineNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WineNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = WineNN()\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 50\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                preds = model(xb)\n",
        "                _, predicted = torch.max(preds, 1)\n",
        "                total += yb.size(0)\n",
        "                correct += (predicted == yb).sum().item()\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        accuracy = correct / total\n",
        "        print(f\"Epoch {epoch:2d}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpO0tCOWFr7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1\n",
        "# Build a CNN using Keras\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation = 'relu', input_shape(28, 28, 1)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(10, activation = 'softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "g8xP9ToJTLJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2\n",
        "# Equivalent PyTorch code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # 28x28 → 28x28\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)                                 # 28x28 → 14x14\n",
        "        self.flattened_size = 32 * 14 * 14\n",
        "        self.fc = nn.Linear(self.flattened_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))                      # Conv + ReLU\n",
        "        x = self.pool(x)                              # MaxPooling\n",
        "        x = x.view(-1, self.flattened_size)           # Flatten\n",
        "        x = F.softmax(self.fc(x), dim=1)              # Dense + Softmax\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "model = CNNModel()\n"
      ],
      "metadata": {
        "id": "7eB2CVM7WAeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working with Fashion MNIST dataset**\n",
        "\n",
        "Link: https://www.tensorflow.org/datasets/catalog/fashion_mnist\n",
        "\n",
        "**Description:**\n",
        "\n",
        "The Fashion MNIST dataset is a collection of 70,000 grayscale images of 28×28 pixels, depicting 10 categories of clothing items such as shirts, shoes, bags, and dresses. Designed as a more complex alternative to the classic MNIST handwritten digit dataset, Fashion MNIST offers richer visual features while maintaining the same image size and format, making it ideal for benchmarking machine learning algorithms. Widely used for testing image classification models, it helps researchers evaluate their techniques on realistic yet manageable data. The dataset is conveniently accessible through TensorFlow’s Keras API, facilitating easy integration into deep learning workflows.\n",
        "\n",
        "**PART 01: Building Blocks**\n",
        "\n",
        "Apply:\n",
        "* A manual convolution using NumPy (for intuition)\n",
        "* A ReLU function manually\n",
        "* Max pooling manually\n",
        "\n",
        "\n",
        "**PART 02: Build a CNN**\n",
        "\n",
        "This CNN includes:\n",
        "\n",
        "* Convolution + ReLU\n",
        "* Max Pooling\n",
        "* Flatten\n",
        "* Dense layers\n",
        "* Output layer with softmax\n",
        "\n",
        "**Part 03: Visulaize and interpret**\n",
        "\n",
        "Here, we,\n",
        "* Visualize filters/feature maps from the first convolution layer.\n",
        "* Show how the network progressively focuses on shapes, edges, etc.\n",
        "\n",
        "\n",
        "**Part 04: Experiment**\n",
        "\n",
        "Explore and work aound:\n",
        "* Change number of filters, kernel size, or activation function.\n",
        "* Observe how performance or feature maps change.\n",
        "* Try adding an extra convolution + pooling layer."
      ],
      "metadata": {
        "id": "9uFlj7zwXuxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "#                                 PART 01\n",
        "# ------------------------------------------------------------------------------\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load Fashion MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "image = train_images[0]  # Use the first image\n",
        "\n",
        "# --- Utility: Normalize output to [0, 1] for better visualization ---\n",
        "def normalize(img):\n",
        "    img = img - np.min(img)\n",
        "    max_val = np.max(img)\n",
        "    if max_val != 0:\n",
        "        img = img / max_val\n",
        "    return img\n",
        "\n",
        "# --- Manual Convolution ---\n",
        "def manual_convolution(image, kernel, stride=1):\n",
        "    kh, kw = kernel.shape\n",
        "    h, w = image.shape\n",
        "    out_h = (h - kh) // stride + 1\n",
        "    out_w = (w - kw) // stride + 1\n",
        "    output = np.zeros((out_h, out_w))\n",
        "\n",
        "    for y in range(out_h):\n",
        "        for x in range(out_w):\n",
        "            patch = image[y*stride:y*stride+kh, x*stride:x*stride+kw]\n",
        "            output[y, x] = np.sum(patch * kernel)\n",
        "    return output\n",
        "\n",
        "# --- Manual ReLU ---\n",
        "def manual_relu(feature_map):\n",
        "    return np.maximum(0, feature_map)\n",
        "\n",
        "# --- Manual Max Pooling ---\n",
        "def manual_max_pooling(feature_map, size=2, stride=2):\n",
        "    h, w = feature_map.shape\n",
        "    out_h = (h - size) // stride + 1\n",
        "    out_w = (w - size) // stride + 1\n",
        "    pooled = np.zeros((out_h, out_w))\n",
        "\n",
        "    for y in range(out_h):\n",
        "        for x in range(out_w):\n",
        "            patch = feature_map[y*stride:y*stride+size, x*stride:x*stride+size]\n",
        "            pooled[y, x] = np.max(patch)\n",
        "    return pooled\n",
        "\n",
        "# Define edge detection kernel\n",
        "kernel = np.array([\n",
        "    [-1, 0, 1],\n",
        "    [-2, 0, 2],\n",
        "    [-1, 0, 1]\n",
        "])\n",
        "\n",
        "# --- Apply Steps ---\n",
        "conv_output = manual_convolution(image, kernel)\n",
        "relu_output = manual_relu(conv_output)\n",
        "pooled_output = manual_max_pooling(relu_output)\n",
        "\n",
        "# --- Plot All Steps ---\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(normalize(conv_output), cmap='gray')\n",
        "plt.title(\"Convolution\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(normalize(relu_output), cmap='gray')\n",
        "plt.title(\"ReLU\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(normalize(pooled_output), cmap='gray')\n",
        "plt.title(\"Max Pool\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iPSi_GuMXpT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "#                                 PART 02\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize images to [0,1] and reshape for CNN input (28x28x1)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Define Mini CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "YzwXw3R8aHwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "#                                 PART 03\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Build model using Functional API\n",
        "# ---------------------------------------\n",
        "input_layer = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', name=\"conv1\")(input_layer)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output_layer = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile and train briefly\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train[:10000], y_train[:10000], epochs=2, batch_size=64)\n",
        "\n",
        "# -------------------------------\n",
        "# 🔍 Part A: Visualize Filters\n",
        "# -------------------------------\n",
        "filters, biases = model.get_layer(\"conv1\").get_weights()\n",
        "filters = (filters - filters.min()) / (filters.max() - filters.min())  # Normalize\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "for i in range(6):\n",
        "    f = filters[:, :, 0, i]\n",
        "    plt.subplot(1, 6, i+1)\n",
        "    plt.imshow(f, cmap='gray')\n",
        "    plt.title(f'Filter {i+1}')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"First Conv Layer Filters\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 🔍 Part B: Visualize Feature Maps\n",
        "# -------------------------------\n",
        "# New model that outputs after conv1\n",
        "feature_extractor = Model(inputs=model.input, outputs=model.get_layer(\"conv1\").output)\n",
        "\n",
        "# Pick one image\n",
        "sample_img = x_train[0:1]\n",
        "feature_maps = feature_extractor.predict(sample_img)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i in range(6):\n",
        "    plt.subplot(1, 6, i+1)\n",
        "    plt.imshow(feature_maps[0, :, :, i], cmap='gray')\n",
        "    plt.title(f'Map {i+1}')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(\"Feature Maps after Conv1\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QJLSg5evcgl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "#                                 PART 04\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Experiment with the code\n",
        "# Can we improve the accuracy?\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize images to [0,1] and reshape for CNN input (28x28x1)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Define Mini CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "OZreqFmueWj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using CNN for time series data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Download Air Passengers data CSV\n",
        "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\n",
        "data = pd.read_csv(url, usecols=[1])\n",
        "passengers = data.values.astype(float)\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "passengers_scaled = scaler.fit_transform(passengers)\n",
        "\n",
        "# Create sequences for supervised learning\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "SEQ_LENGTH = 12  # Use 12 months to predict next month\n",
        "X, y = create_sequences(passengers_scaled, SEQ_LENGTH)\n",
        "\n",
        "# Reshape X for Conv1D: (samples, time steps, features)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Split into train and test sets (80% train)\n",
        "split = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(SEQ_LENGTH, 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform to original scale\n",
        "y_test_inv = scaler.inverse_transform(y_test)\n",
        "y_pred_inv = scaler.inverse_transform(y_pred)\n",
        "\n",
        "# Plot true vs predicted\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(y_test_inv, label='True')\n",
        "plt.plot(y_pred_inv, label='Predicted')\n",
        "plt.title(\"Air Passengers Forecast: True vs Predicted\")\n",
        "plt.xlabel(\"Time steps\")\n",
        "plt.ylabel(\"Number of Passengers\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIVQjMhefZx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Architectures**\n",
        "\n",
        "LeNet, AlexNet, and VGG are landmark CNN architectures that shaped the evolution of deep learning. LeNet (1998), developed by Yann LeCun, was designed for digit recognition and introduced the core CNN structure with convolution, pooling, and fully connected layers. AlexNet (2012) revived deep learning by winning the ImageNet challenge using ReLU activations, dropout, and GPU acceleration. It marked a significant leap in performance. VGG (2014) emphasized simplicity and depth by stacking small 3×3 convolution filters, creating very deep yet uniform networks. Each model reflects a step forward in scalability, performance, and design clarity in visual recognition tasks."
      ],
      "metadata": {
        "id": "WAkrte1sd8Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet Architecture\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5),\n",
        "            nn.Tanh(),\n",
        "            nn.AvgPool2d(2)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(16 * 4 * 4, 120),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc_layers(x)\n"
      ],
      "metadata": {
        "id": "Mu2DWma9eV_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AlexNet Architecture\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.Conv2d(64, 192, 5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "            nn.Conv2d(192, 384, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, 2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "yWrLAB2Tn0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG Architecture\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):  # Customizable output\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ACsi5duQrLK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working with CIFAR-10 dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# CIFAR-10 dataset (no resizing)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Simple CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # 32x32x32\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),               # 32x16x16\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1), # 64x16x16\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),               # 64x8x8\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1), # 128x8x8\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)                # 128x4x4\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128*4*4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss={running_loss/len(trainloader):.4f}, Accuracy={100.*correct/total:.2f}%\")\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# Train for 10 epochs\n",
        "for epoch in range(1, 11):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "id": "JckzGOzBQK_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 with AlexNet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Transform including resize for AlexNet input size\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))  # Imagenet mean/std\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Load pretrained AlexNet\n",
        "model = alexnet(pretrained=True)\n",
        "\n",
        "# Replace last layer for CIFAR-10 (10 classes)\n",
        "model.classifier[6] = nn.Linear(4096, 10)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower LR for finetuning\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    print(f\"Epoch {epoch}: Loss={running_loss/len(trainloader):.4f}, Accuracy={100.*correct/total:.2f}%\")\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    print(f\"Test Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# Train for 10 epochs\n",
        "for epoch in range(1, 11):\n",
        "    train(epoch)\n",
        "test()\n"
      ],
      "metadata": {
        "id": "sOr1lzf9Sq37"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}